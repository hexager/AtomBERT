{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-29T05:41:10.163748Z","iopub.execute_input":"2025-03-29T05:41:10.164192Z","iopub.status.idle":"2025-03-29T05:41:10.169458Z","shell.execute_reply.started":"2025-03-29T05:41:10.164162Z","shell.execute_reply":"2025-03-29T05:41:10.168455Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"from transformers import BertTokenizerFast\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import load_dataset\nfrom datasets import Dataset as HFDataset\nimport torch\nimport torch.nn.functional as F\nimport math\nimport torch.nn as nn\nimport torch.optim as optim\nfrom tqdm.notebook import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T05:41:10.170986Z","iopub.execute_input":"2025-03-29T05:41:10.171295Z","iopub.status.idle":"2025-03-29T05:41:16.098715Z","shell.execute_reply.started":"2025-03-29T05:41:10.171265Z","shell.execute_reply":"2025-03-29T05:41:16.097697Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"!pip install datasets\n!pip install wandb\n!wandb login 9bd511b60253680868c24c9a01b41ca049a73dd6","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T05:41:16.100645Z","iopub.execute_input":"2025-03-29T05:41:16.101197Z","iopub.status.idle":"2025-03-29T05:41:26.325910Z","shell.execute_reply.started":"2025-03-29T05:41:16.101162Z","shell.execute_reply":"2025-03-29T05:41:26.324642Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.3.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.17.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.12)\nRequirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.29.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->datasets) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->datasets) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.19.1)\nRequirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\nRequirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\nRequirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\nRequirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\nRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\nRequirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.11.0a2)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.2)\nRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\nRequirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.19.2)\nRequirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.4)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (75.1.0)\nRequirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.12.2)\nRequirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\nRequirement already satisfied: pydantic-core==2.29.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.6->wandb) (2.29.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2025.1.31)\nRequirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\nsentence = \"By the way, we ball\"\ntokens = tokenizer.tokenize(sentence)\nprint(tokens)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T05:41:26.327836Z","iopub.execute_input":"2025-03-29T05:41:26.328173Z","iopub.status.idle":"2025-03-29T05:41:30.864618Z","shell.execute_reply.started":"2025-03-29T05:41:26.328144Z","shell.execute_reply":"2025-03-29T05:41:30.863747Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f88e36a52e747a082e15042303b6909"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a3b83fb324044e28b0ef3736a167fbd4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"12c7b988f03b4ce7848b7421688d36a9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8946ab28b2604f878ec9ef8575e52420"}},"metadata":{}},{"name":"stdout","text":"['by', 'the', 'way', ',', 'we', 'ball']\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"mnli_dataset = load_dataset(\"multi_nli\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T05:41:30.865486Z","iopub.execute_input":"2025-03-29T05:41:30.866013Z","iopub.status.idle":"2025-03-29T05:41:35.465546Z","shell.execute_reply.started":"2025-03-29T05:41:30.865974Z","shell.execute_reply":"2025-03-29T05:41:35.464794Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/8.89k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2dd1398ed6d74ad0b17f1cc4a357d210"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/214M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b48c9f12122847009ff45ad727e76ef4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(…)alidation_matched-00000-of-00001.parquet:   0%|          | 0.00/4.94M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6233177107ca4e7bbf750f3cde6a5d57"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(…)dation_mismatched-00000-of-00001.parquet:   0%|          | 0.00/5.10M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6102c1c982f34974917418839df0571f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/392702 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"615701fabf2541aba37549437abe55e7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation_matched split:   0%|          | 0/9815 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4818a3c400d049d0954bc34f755f7a28"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation_mismatched split:   0%|          | 0/9832 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c92e88f333084f51960e94814d583dd8"}},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"print(mnli_dataset['train'][0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T05:41:35.466430Z","iopub.execute_input":"2025-03-29T05:41:35.466667Z","iopub.status.idle":"2025-03-29T05:41:35.471996Z","shell.execute_reply.started":"2025-03-29T05:41:35.466646Z","shell.execute_reply":"2025-03-29T05:41:35.470825Z"}},"outputs":[{"name":"stdout","text":"{'promptID': 31193, 'pairID': '31193n', 'premise': 'Conceptually cream skimming has two basic dimensions - product and geography.', 'premise_binary_parse': '( ( Conceptually ( cream skimming ) ) ( ( has ( ( ( two ( basic dimensions ) ) - ) ( ( product and ) geography ) ) ) . ) )', 'premise_parse': '(ROOT (S (NP (JJ Conceptually) (NN cream) (NN skimming)) (VP (VBZ has) (NP (NP (CD two) (JJ basic) (NNS dimensions)) (: -) (NP (NN product) (CC and) (NN geography)))) (. .)))', 'hypothesis': 'Product and geography are what make cream skimming work. ', 'hypothesis_binary_parse': '( ( ( Product and ) geography ) ( ( are ( what ( make ( cream ( skimming work ) ) ) ) ) . ) )', 'hypothesis_parse': '(ROOT (S (NP (NN Product) (CC and) (NN geography)) (VP (VBP are) (SBAR (WHNP (WP what)) (S (VP (VBP make) (NP (NP (NN cream)) (VP (VBG skimming) (NP (NN work)))))))) (. .)))', 'genre': 'government', 'label': 1}\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"class MNLIDataset(Dataset):\n    def __init__(self, data, tokenizer, max_length):\n        self.data = data\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n    def __len__(self):\n        return len(self.data)\n    def  __getitem__(self, idx):\n        example = self.data[idx]\n        premise = example['premise']\n        hypothesis = example['hypothesis']\n        label = example['label']\n        encoded_pair = self.tokenizer.encode_plus(premise, hypothesis, max_length=self.max_length, padding='max_length', truncation=True,return_tensors='pt')\n        input_ids = encoded_pair['input_ids'].squeeze(0)\n        attention_mask = encoded_pair['attention_mask'].squeeze(0)\n        # Keeping this commented out for now, maybe not very essential for encoder only models? Investigate further...\n        # token_type_ids = encoded_pair.get('token_type_ids', torch.zeros_like(input_ids))\n        return {'input_ids': input_ids,'attention_mask': attention_mask,# 'token_type_ids': token_type_ids,\n'labels': torch.tensor(label)}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T05:41:35.472944Z","iopub.execute_input":"2025-03-29T05:41:35.473261Z","iopub.status.idle":"2025-03-29T05:41:35.555228Z","shell.execute_reply.started":"2025-03-29T05:41:35.473238Z","shell.execute_reply":"2025-03-29T05:41:35.554045Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"train_data = mnli_dataset[\"train\"]\nmax_seq_length = 128\ntrain_dataset = MNLIDataset(train_data, tokenizer, max_seq_length)\nprint(f\"Size of training dataset: {len(train_dataset)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T05:41:35.556372Z","iopub.execute_input":"2025-03-29T05:41:35.556710Z","iopub.status.idle":"2025-03-29T05:41:35.573499Z","shell.execute_reply.started":"2025-03-29T05:41:35.556676Z","shell.execute_reply":"2025-03-29T05:41:35.572649Z"}},"outputs":[{"name":"stdout","text":"Size of training dataset: 392702\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"sample = train_dataset[0]\nprint(sample['input_ids'].shape)\nprint(sample['attention_mask'].shape)\nprint(sample['labels'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T05:41:35.575957Z","iopub.execute_input":"2025-03-29T05:41:35.576244Z","iopub.status.idle":"2025-03-29T05:41:35.626784Z","shell.execute_reply.started":"2025-03-29T05:41:35.576222Z","shell.execute_reply":"2025-03-29T05:41:35.625985Z"}},"outputs":[{"name":"stdout","text":"torch.Size([128])\ntorch.Size([128])\ntensor(1)\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"class MultiHeadSelfAttention(nn.Module):\n    def __init__(self, hidden_size, num_attention_heads, dropout_rate):\n        super().__init__()\n        self.num_heads = num_attention_heads\n        self.head_dim = hidden_size // num_attention_heads\n        assert self.head_dim * self.num_heads == hidden_size\n\n        self.query = nn.Linear(hidden_size, hidden_size)\n        self.key = nn.Linear(hidden_size, hidden_size)\n        self.value = nn.Linear(hidden_size, hidden_size)\n\n        self.dropout = nn.Dropout(dropout_rate)\n        self.output = nn.Linear(hidden_size, hidden_size)\n    def forward(self, query, key, value, mask=None):\n        batch_size = query.size(0)\n        seq_len_q, seq_len_k, seq_len_v = query.size(1), key.size(1), value.size(1)\n        query = self.query(query)\n        key = self.key(key)\n        value = self.value(value)\n        query = query.view(batch_size, seq_len_q, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n        key = key.view(batch_size, seq_len_k, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n        value = value.view(batch_size, seq_len_v, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n        attention_scores = torch.matmul(query, key.transpose(-2,-1))\n        attention_scores = attention_scores/(self.head_dim**0.5)\n        mask = mask.unsqueeze(1).unsqueeze(2)\n        if mask is not None:\n            attention_scores = attention_scores.masked_fill(mask==0, float('-inf'))\n        attention_weights = F.softmax(attention_scores,dim=-1)\n        scaled_attention = torch.matmul(attention_weights, value)\n        scaled_attention = scaled_attention.permute(0, 2, 1, 3).contiguous().view(batch_size, seq_len_q, self.num_heads * self.head_dim)\n        output = self.output(scaled_attention)\n        return output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T05:41:35.628076Z","iopub.execute_input":"2025-03-29T05:41:35.628334Z","iopub.status.idle":"2025-03-29T05:41:35.636587Z","shell.execute_reply.started":"2025-03-29T05:41:35.628314Z","shell.execute_reply":"2025-03-29T05:41:35.635807Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"class FeedForwardNetwork(nn.Module):\n    def __init__(self, hidden_size, intermediate_size, dropout_rate):\n        super().__init__()\n        self.dense1 = nn.Linear(hidden_size, intermediate_size)\n        self.relu = nn.ReLU()\n        self.dense2 = nn.Linear(intermediate_size, hidden_size)\n        self.dropout = nn.Dropout(dropout_rate)\n\n    def forward(self, x):\n        x = F.relu(self.dense1(x))\n        x = self.dropout(x)\n        x = self.dense2(x)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T05:41:35.637505Z","iopub.execute_input":"2025-03-29T05:41:35.637757Z","iopub.status.idle":"2025-03-29T05:41:35.656477Z","shell.execute_reply.started":"2025-03-29T05:41:35.637737Z","shell.execute_reply":"2025-03-29T05:41:35.655551Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"class TransformerEncoderLayer(nn.Module):\n    def __init__(self, hidden_size, num_attention_heads, intermediate_size, dropout_rate):\n        super().__init__()\n        self.self_attention = MultiHeadSelfAttention(hidden_size, num_attention_heads, dropout_rate)\n        self.feed_forward = FeedForwardNetwork(hidden_size, intermediate_size, dropout_rate)\n        self.norm1 = nn.LayerNorm(hidden_size)\n        self.norm2 = nn.LayerNorm(hidden_size)\n        self.dropout = nn.Dropout(dropout_rate)\n\n    def forward(self, x, mask):\n        attention_output = self.self_attention(x,x, x, mask)\n        normed_output1 = self.norm1(attention_output + x)\n        ff_output = self.feed_forward(normed_output1)\n        final_output = self.norm2(ff_output + normed_output1)\n        return final_output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T05:41:35.657392Z","iopub.execute_input":"2025-03-29T05:41:35.657701Z","iopub.status.idle":"2025-03-29T05:41:35.672955Z","shell.execute_reply.started":"2025-03-29T05:41:35.657677Z","shell.execute_reply":"2025-03-29T05:41:35.672174Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"class FactorizedEmbedding(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_size, type_vocab_size=2):\n        super().__init__()\n        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n        self.token_type_embeddings = nn.Embedding(type_vocab_size, embedding_dim)\n        self.projection = nn.Linear(embedding_dim, hidden_size)\n\n    def forward(self, input_ids, token_type_ids=None):\n        factor_embeds = self.word_embeddings(input_ids)\n        if token_type_ids is None:\n            token_type_ids = torch.zeros_like(input_ids)\n        token_type_embeds = self.token_type_embeddings(token_type_ids)\n        # Sum the embeddings\n        embeds = factor_embeds + token_type_embeds        \n        project_embeds = self.projection(embeds)\n        return project_embeds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T05:41:35.673837Z","iopub.execute_input":"2025-03-29T05:41:35.674108Z","iopub.status.idle":"2025-03-29T05:41:35.686986Z","shell.execute_reply.started":"2025-03-29T05:41:35.674064Z","shell.execute_reply":"2025-03-29T05:41:35.686256Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"class PositionalEncoding(nn.Module):\n    def __init__(self, hidden_size, max_seq_length, dropout_rate):\n        super().__init__()\n        self.dropout = nn.Dropout(dropout_rate)\n\n        position = torch.arange(0, max_seq_length).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, hidden_size, 2) * -(math.log(10000.0) / hidden_size))\n        pe = torch.zeros(max_seq_length, 1, hidden_size)\n        pe[:, 0, 0::2] = torch.sin(position * div_term)\n        pe[:, 0, 1::2] = torch.cos(position * div_term)\n        self.register_buffer('pe', pe) # Store as a buffer (not a learnable parameter)\n\n    def forward(self, x):\n        seq_length = x.size(1)\n        pe = self.pe[:seq_length].squeeze(1)\n        x = x + pe.unsqueeze(0)\n        x = self.dropout(x)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T05:41:35.687972Z","iopub.execute_input":"2025-03-29T05:41:35.688314Z","iopub.status.idle":"2025-03-29T05:41:35.703437Z","shell.execute_reply.started":"2025-03-29T05:41:35.688283Z","shell.execute_reply":"2025-03-29T05:41:35.702691Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"class AtomBERT(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers, num_attention_heads, intermediate_size, num_classes, max_seq_length, dropout_rate, type_vocab_size=2):\n        super().__init__()\n        self.embedding = FactorizedEmbedding(vocab_size, embedding_dim, hidden_size, type_vocab_size)\n        self.positional_encoding = PositionalEncoding(hidden_size, max_seq_length, dropout_rate)\n        self.encoder_layer = TransformerEncoderLayer(hidden_size, num_attention_heads, intermediate_size, dropout_rate)\n        self.num_layers = num_layers\n        self.dropout = nn.Dropout(dropout_rate)\n        #self.classifier = nn.Linear(hidden_size, num_classes)\n        self.hidden_size = hidden_size\n\n    def forward(self, input_ids, attention_mask, token_type_ids=None):\n        embeddings = self.embedding(input_ids, token_type_ids)\n        embeddings = self.positional_encoding(embeddings)\n\n        # Share the encoder layer across all layers\n        encoder_output = embeddings\n        for _ in range(self.num_layers):\n            encoder_output = self.encoder_layer(encoder_output, attention_mask)\n\n        pooled_output = encoder_output[:, 0, :] # Shape: (batch_size, hidden_size)\n        pooled_output = self.dropout(pooled_output)\n        #logits = self.classifier(pooled_output)\n        return encoder_output, pooled_output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T05:41:35.704277Z","iopub.execute_input":"2025-03-29T05:41:35.704498Z","iopub.status.idle":"2025-03-29T05:41:35.716513Z","shell.execute_reply.started":"2025-03-29T05:41:35.704479Z","shell.execute_reply":"2025-03-29T05:41:35.715733Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"class AtomBERTConfig:\n    def __init__(\n        self,\n        vocab_size,\n        embedding_dim=128,\n        hidden_size=768,\n        num_layers=4,\n        num_attention_heads=4,\n        intermediate_size=1500,\n        num_classes=2, # For SOP\n        max_seq_length=128,\n        dropout_rate=0.1,\n        type_vocab_size = 2\n    ):\n        self.vocab_size = vocab_size\n        self.embedding_dim = embedding_dim\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.num_attention_heads = num_attention_heads\n        self.intermediate_size = intermediate_size\n        self.num_classes = num_classes\n        self.max_seq_length = max_seq_length\n        self.dropout_rate = dropout_rate\n        self.type_vocab_size = type_vocab_size","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T05:41:35.717363Z","iopub.execute_input":"2025-03-29T05:41:35.717582Z","iopub.status.idle":"2025-03-29T05:41:35.732754Z","shell.execute_reply.started":"2025-03-29T05:41:35.717564Z","shell.execute_reply":"2025-03-29T05:41:35.731858Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"class AtomBERTForPretraining(nn.Module): # Renamed for clarity\n    def __init__(self, config):\n        super().__init__()\n        self.config = config # Store the config object\n        self.bert = AtomBERT(\n            vocab_size=config.vocab_size,\n            embedding_dim=config.embedding_dim,\n            hidden_size=config.hidden_size,\n            num_layers=config.num_layers,\n            num_attention_heads=config.num_attention_heads,\n            intermediate_size=config.intermediate_size,\n            num_classes=2, # For SOP\n            max_seq_length=config.max_seq_length,\n            dropout_rate=config.dropout_rate,\n            type_vocab_size=config.type_vocab_size if hasattr(config, 'type_vocab_size') else 2\n        )\n        self.mlm_head = nn.Sequential(\n        nn.Linear(config.hidden_size, config.hidden_size),\n        nn.LayerNorm(config.hidden_size),\n        nn.Linear(config.hidden_size, config.vocab_size)\n        )\n        self.sop_head = nn.Linear(config.hidden_size, 2)\n\n\n\n    def forward(self, input_ids, attention_mask, masked_labels=None, sentence_order_labels=None, token_type_ids=None):\n        outputs = self.bert(input_ids, attention_mask, token_type_ids) # Shape: (batch_size, seq_len, hidden_size)\n        encoder_output = outputs[0]\n        pooled_output = outputs[1]\n        # MLM Prediction\n        prediction_logits_mlm = self.mlm_head(encoder_output) # Shape: (batch_size, seq_len, vocab_size)\n\n        # SOP Prediction (using the pooled output - we might need to adjust this)\n        #pooled_output = outputs[:, 0, :] # Taking the [CLS] token representation\n        prediction_logits_sop = self.sop_head(pooled_output) # Shape: (batch_size, 2)\n\n        total_loss = None\n        if masked_labels is not None and sentence_order_labels is not None:\n            loss_fct_mlm = nn.CrossEntropyLoss()\n            masked_loss = loss_fct_mlm(prediction_logits_mlm.view(-1, self.config.vocab_size), masked_labels.view(-1))\n\n            loss_fct_sop = nn.CrossEntropyLoss()\n            sop_loss = loss_fct_sop(prediction_logits_sop.view(-1, 2), sentence_order_labels.view(-1))\n\n            total_loss = masked_loss + sop_loss # You might want to weigh these differently\n\n        elif masked_labels is not None:\n            loss_fct_mlm = nn.CrossEntropyLoss()\n            total_loss = loss_fct_mlm(prediction_logits_mlm.view(-1, self.config.vocab_size), masked_labels.view(-1))\n\n        elif sentence_order_labels is not None:\n            loss_fct_sop = nn.CrossEntropyLoss()\n            total_loss = loss_fct_sop(prediction_logits_sop.view(-1, 2), sentence_order_labels.view(-1))\n\n        return total_loss, prediction_logits_mlm, prediction_logits_sop","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T05:41:35.733705Z","iopub.execute_input":"2025-03-29T05:41:35.734049Z","iopub.status.idle":"2025-03-29T05:41:35.749675Z","shell.execute_reply.started":"2025-03-29T05:41:35.734017Z","shell.execute_reply":"2025-03-29T05:41:35.748916Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"wiki_text_dataset = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\")\nprint(wiki_text_dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T05:41:35.750607Z","iopub.execute_input":"2025-03-29T05:41:35.750890Z","iopub.status.idle":"2025-03-29T05:41:42.485332Z","shell.execute_reply.started":"2025-03-29T05:41:35.750849Z","shell.execute_reply":"2025-03-29T05:41:42.484384Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/10.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a81e83d873954b7695bc61b24750e6ee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/733k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c3a99bf805bc4005bbd94ee399cd8112"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00002.parquet:   0%|          | 0.00/157M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ad1cb8cbbc544c38ec1853e0c4b7e62"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00001-of-00002.parquet:   0%|          | 0.00/157M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2eed2fa0307e4749b2dd6cb47a321455"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/657k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"041d2fe5298141cbb23463bb6a7e487e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b496409a5884a08b0e8892c30e6b238"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/1801350 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8cb6b146563c436d8a299f408de9ca12"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b38ac15caf124a6b937eae477c1c9804"}},"metadata":{}},{"name":"stdout","text":"DatasetDict({\n    test: Dataset({\n        features: ['text'],\n        num_rows: 4358\n    })\n    train: Dataset({\n        features: ['text'],\n        num_rows: 1801350\n    })\n    validation: Dataset({\n        features: ['text'],\n        num_rows: 3760\n    })\n})\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"vocab_size = tokenizer.vocab_size\nlearning_rate = 5e-5\n\n# Move the model to the device (GPU if available)\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n# Recall or re-initialize your tokenizer if needed\nvocab_size = tokenizer.vocab_size\n\n# Create the configuration\nconfig = AtomBERTConfig(vocab_size=vocab_size) # You can customize other parameters here if needed\n\n# Instantiate the AtomBERTForPretraining model\nmodelpretrain = AtomBERTForPretraining(config)\n\nprint(f\"Model instantiated with {config}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T05:41:42.486170Z","iopub.execute_input":"2025-03-29T05:41:42.486409Z","iopub.status.idle":"2025-03-29T05:41:42.878377Z","shell.execute_reply.started":"2025-03-29T05:41:42.486388Z","shell.execute_reply":"2025-03-29T05:41:42.877415Z"}},"outputs":[{"name":"stdout","text":"Model instantiated with <__main__.AtomBERTConfig object at 0x7f65790054b0>\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"def count_parameters(model):\n    total_params = 0\n    trainable_params = 0\n    non_trainable_params = 0\n\n    for name, param in model.named_parameters():\n        num_params = param.numel()\n        # if requires_grad is True, then it is a trainable parameter\n        if param.requires_grad:\n            trainable_params += num_params\n        else:\n            non_trainable_params += num_params\n        total_params += num_params\n\n    print(f\"Total Parameters: {total_params:,}\")\n    print(f\"Trainable Parameters: {trainable_params:,}\")\n    print(f\"Non-trainable Parameters: {non_trainable_params:,}\")\nprint(\"Basic Model: \\n\")\ncount_parameters(modelpretrain.bert)\nprint(\"Total Pretraining Model: \\n\")\ncount_parameters(modelpretrain)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T05:41:42.879395Z","iopub.execute_input":"2025-03-29T05:41:42.879682Z","iopub.status.idle":"2025-03-29T05:41:43.204662Z","shell.execute_reply.started":"2025-03-29T05:41:42.879651Z","shell.execute_reply":"2025-03-29T05:41:43.203800Z"}},"outputs":[{"name":"stdout","text":"Basic Model: \n\nTotal Parameters: 8,677,852\nTrainable Parameters: 8,677,852\nNon-trainable Parameters: 0\nTotal Pretraining Model: \n\nTotal Parameters: 32,742,936\nTrainable Parameters: 32,742,936\nNon-trainable Parameters: 0\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"# Start a new wandb run to track this script.\nimport wandb\nrun = wandb.init(\n    # Set the wandb entity where your project will be logged (generally your team name).\n    entity=\"hexager-manipal\",\n    # Set the wandb project where this run will be logged.\n    project=\"MRM-transform\",\n    # Track hyperparameters and run metadata.\n    config={\n        \"learning_rate\": 0.02,\n        \"architecture\": \"AlBERT-32M\",\n        \"dataset\": \"WikiText-103\",\n        \"epochs\": 4,\n    },\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T05:41:43.205531Z","iopub.execute_input":"2025-03-29T05:41:43.205819Z","iopub.status.idle":"2025-03-29T05:41:56.005306Z","shell.execute_reply.started":"2025-03-29T05:41:43.205786Z","shell.execute_reply":"2025-03-29T05:41:56.004343Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhexager\u001b[0m (\u001b[33mhexager-manipal\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250329_054149-xh9bevz3</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/hexager-manipal/MRM-transform/runs/xh9bevz3' target=\"_blank\">curious-music-1</a></strong> to <a href='https://wandb.ai/hexager-manipal/MRM-transform' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/hexager-manipal/MRM-transform' target=\"_blank\">https://wandb.ai/hexager-manipal/MRM-transform</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/hexager-manipal/MRM-transform/runs/xh9bevz3' target=\"_blank\">https://wandb.ai/hexager-manipal/MRM-transform/runs/xh9bevz3</a>"},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"\n# Define parameters (ensure max_seq_length and batch_size are defined)\nmax_seq_length = 128  # Example value, adjust as needed\nbatch_size = 16      # Example value, adjust as needed\nnum_epochs = 4\nlearning_rate = 1e-4\nweight_decay = 0.01\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n\n# Load tokenizer\ntokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n\n# Load dataset\nwiki_text_dataset = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\")\n\n# Tokenize dataset\ndef tokenize_function(examples):\n    return tokenizer(examples[\"text\"], truncation=True, padding='max_length', max_length=max_seq_length)\ntokenized_train_dataset = wiki_text_dataset[\"train\"].map(tokenize_function, batched=True)\n\n# Prepare MLM data\ndef prepare_mlm_data(examples):\n    input_ids = torch.tensor(examples[\"input_ids\"])\n    batch_size_local = len(input_ids)\n    sequence_length = len(input_ids[0])\n    mask_prob = 0.15\n    mask_token_id = tokenizer.mask_token_id\n    pad_token_id = tokenizer.pad_token_id\n    cls_token_id = tokenizer.cls_token_id\n    sep_token_id = tokenizer.sep_token_id\n\n    rand = torch.rand(batch_size_local, sequence_length)\n    mask = (rand < mask_prob) & (input_ids != cls_token_id) & (input_ids != sep_token_id) & (input_ids != pad_token_id)\n\n    labels = input_ids.clone()\n    labels[~mask] = -100\n    inputs = input_ids.clone()\n    inputs[mask] = mask_token_id\n\n    return {\"input_ids\": inputs, \"labels\": labels}\nmlm_tokenized_train_dataset = tokenized_train_dataset.map(\n    prepare_mlm_data,\n    batched=True,\n    remove_columns=[\"text\", \"token_type_ids\"]\n)\n\n# Prepare SOP data\nimport re\nfrom datasets import Dataset as HFDataset\n\ndef split_into_sentences_robust(text):\n    sentence_pattern = r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?|!)\\s+'\n    sentences = re.split(sentence_pattern, text)\n    return [s.strip() for s in sentences if s.strip()]\n\nsop_texts_limited = []\nsop_labels_limited = []\ncls_token = tokenizer.cls_token\nsep_token = tokenizer.sep_token\nmax_pairs_per_document = 5\n\nfor example in wiki_text_dataset[\"train\"]:\n    text = example[\"text\"]\n    sentences = split_into_sentences_robust(text)\n    pairs_count = 0\n    for i in range(len(sentences) - 1):\n        if pairs_count >= max_pairs_per_document:\n            break\n        sentence_a = sentences[i]\n        sentence_b = sentences[i + 1]\n        sop_texts_limited.append(cls_token + \" \" + sentence_a + \" \" + sep_token + \" \" + sentence_b)\n        sop_labels_limited.append(0)\n        pairs_count += 1\n        if pairs_count >= max_pairs_per_document:\n            break\n        sop_texts_limited.append(cls_token + \" \" + sentence_b + \" \" + sep_token + \" \" + sentence_a)\n        sop_labels_limited.append(1)\n        pairs_count += 1\n\nsop_dataset = HFDataset.from_dict({\"text\": sop_texts_limited, \"sentence_order_labels\": sop_labels_limited})\n\ndef tokenize_sop_function(examples):\n    return tokenizer(examples[\"text\"], truncation=True, padding='max_length', max_length=max_seq_length)\ntokenized_sop_dataset = sop_dataset.map(tokenize_sop_function, batched=True)\n\n# Set format to PyTorch tensors\nmlm_tokenized_train_dataset.set_format(\"torch\")\ntokenized_sop_dataset.set_format(\"torch\")\n\n# DataLoader for MLM dataset\nmlm_dataloader = DataLoader(\n    mlm_tokenized_train_dataset,\n    batch_size=batch_size,\n    shuffle=True\n)\n\n# DataLoader for SOP dataset\nsop_dataloader = DataLoader(\n    tokenized_sop_dataset,\n    batch_size=batch_size,\n    shuffle=True\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T05:41:56.006219Z","iopub.execute_input":"2025-03-29T05:41:56.006483Z","execution_failed":"2025-03-29T05:42:16.313Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1801350 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"modelpretrain.to(device)\nfrom torch.optim import AdamW\noptimizer = AdamW(modelpretrain.parameters(), lr=learning_rate, weight_decay=weight_decay)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for epoch in range(num_epochs):\n    modelpretrain.train()\n    total_mlm_loss = 0\n    total_sop_loss = 0\n    total_steps = min(len(mlm_dataloader), len(sop_dataloader))\n\n    progress_bar = tqdm(range(total_steps), desc=f\"Epoch {epoch+1}/{num_epochs}\")\n\n    for step in progress_bar:\n        try:\n            batch_mlm = next(iter(mlm_dataloader))\n        except StopIteration:\n            print(\"MLM dataloader finished early.\")\n            break\n        try:\n            batch_sop = next(iter(sop_dataloader))\n        except StopIteration:\n            print(\"SOP dataloader finished early.\")\n            break\n\n        # Move MLM batch to device\n        mlm_inputs = {k: batch_mlm[k].to(device) for k in batch_mlm}\n\n        # Move SOP batch to device\n        sop_inputs = {\n                    k: v.to(device) if isinstance(v, torch.Tensor) else v\n                    for k, v in batch_sop.items()\n                    }\n\n        # Zero gradients\n        optimizer.zero_grad()\n\n        # MLM forward pass\n        returned_loss, prediction_logits_mlm, prediction_logits_sop = modelpretrain(\n            input_ids=mlm_inputs['input_ids'].to(device),\n            attention_mask=mlm_inputs['attention_mask'].to(device),\n            masked_labels=mlm_inputs['labels'].to(device)\n        )\n\n        mlm_loss = returned_loss # Now you can use 'returned_loss' which holds the total loss\n\n        # SOP forward pass\n        outputs_sop = modelpretrain(**{\n            'input_ids': sop_inputs['input_ids'].to(device),\n            'attention_mask': sop_inputs['attention_mask'].to(device),\n            'token_type_ids' : sop_inputs.get('token_type_ids', torch.tensor([], device=device)).to(device),\n            'sentence_order_labels': sop_inputs['sentence_order_labels'].to(device)\n        })\n        sop_loss = outputs_sop[0]\n\n        # Combine losses\n        total_loss = mlm_loss + sop_loss\n\n        # Backward pass\n        total_loss.backward()\n\n        # Update parameters\n        optimizer.step()\n\n        total_mlm_loss += mlm_loss.item()\n        total_sop_loss += sop_loss.item()\n\n        progress_bar.set_postfix(mlm_loss=f\"{mlm_loss.item():.4f}\", sop_loss=f\"{sop_loss.item():.4f}\")\n    avg_mlm_loss = total_mlm_loss / total_steps if total_steps > 0 else 0\n    avg_sop_loss = total_sop_loss / total_steps if total_steps > 0 else 0\n    run.log({\"Total Unsupervised Loss\": total_loss})\n    save_path = 'path/to/save/pretrained_model.pth'  # Replace with your desired path\n\n    # Save the state_dict of the model\n    torch.save(modelpretrain.state_dict(), save_path)\n\n    print(f\"Pretrained model weights saved to: {save_path}\")\n    print(f\"Epoch {epoch+1}/{num_epochs} finished, Average MLM Loss: {avg_mlm_loss:.4f}, Average SOP Loss: {avg_sop_loss:.4f}\")\n\nprint(\"Pretraining finished!\")\nrun.finish()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-29T05:42:16.314Z"}},"outputs":[],"execution_count":null}]}