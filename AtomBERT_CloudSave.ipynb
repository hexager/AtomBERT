{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-29T11:04:15.680820Z","iopub.execute_input":"2025-03-29T11:04:15.681142Z","iopub.status.idle":"2025-03-29T11:04:15.685639Z","shell.execute_reply.started":"2025-03-29T11:04:15.681117Z","shell.execute_reply":"2025-03-29T11:04:15.684557Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"from transformers import BertTokenizerFast\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import load_dataset\nfrom datasets import Dataset as HFDataset\nimport torch\nimport torch.nn.functional as F\nimport math\nimport torch.nn as nn\nimport torch.optim as optim\nfrom tqdm.notebook import tqdm\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"wandbpass\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T11:04:15.686772Z","iopub.execute_input":"2025-03-29T11:04:15.687106Z","iopub.status.idle":"2025-03-29T11:04:15.865074Z","shell.execute_reply.started":"2025-03-29T11:04:15.687071Z","shell.execute_reply":"2025-03-29T11:04:15.864369Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"import wandb\nwandb.login(key=secret_value_0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T11:04:15.866365Z","iopub.execute_input":"2025-03-29T11:04:15.866589Z","iopub.status.idle":"2025-03-29T11:04:15.987399Z","shell.execute_reply.started":"2025-03-29T11:04:15.866570Z","shell.execute_reply":"2025-03-29T11:04:15.986677Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":28},{"cell_type":"code","source":"tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\nsentence = \"By the way, we ball\"\ntokens = tokenizer.tokenize(sentence)\nprint(tokens)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T11:04:15.988715Z","iopub.execute_input":"2025-03-29T11:04:15.989043Z","iopub.status.idle":"2025-03-29T11:04:16.181280Z","shell.execute_reply.started":"2025-03-29T11:04:15.989013Z","shell.execute_reply":"2025-03-29T11:04:16.180536Z"}},"outputs":[{"name":"stdout","text":"['by', 'the', 'way', ',', 'we', 'ball']\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"mnli_dataset = load_dataset(\"multi_nli\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T11:04:16.182071Z","iopub.execute_input":"2025-03-29T11:04:16.182337Z","iopub.status.idle":"2025-03-29T11:04:18.897303Z","shell.execute_reply.started":"2025-03-29T11:04:16.182316Z","shell.execute_reply":"2025-03-29T11:04:18.896627Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"print(mnli_dataset['train'][0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T11:04:18.898062Z","iopub.execute_input":"2025-03-29T11:04:18.898324Z","iopub.status.idle":"2025-03-29T11:04:18.903273Z","shell.execute_reply.started":"2025-03-29T11:04:18.898291Z","shell.execute_reply":"2025-03-29T11:04:18.902391Z"}},"outputs":[{"name":"stdout","text":"{'promptID': 31193, 'pairID': '31193n', 'premise': 'Conceptually cream skimming has two basic dimensions - product and geography.', 'premise_binary_parse': '( ( Conceptually ( cream skimming ) ) ( ( has ( ( ( two ( basic dimensions ) ) - ) ( ( product and ) geography ) ) ) . ) )', 'premise_parse': '(ROOT (S (NP (JJ Conceptually) (NN cream) (NN skimming)) (VP (VBZ has) (NP (NP (CD two) (JJ basic) (NNS dimensions)) (: -) (NP (NN product) (CC and) (NN geography)))) (. .)))', 'hypothesis': 'Product and geography are what make cream skimming work. ', 'hypothesis_binary_parse': '( ( ( Product and ) geography ) ( ( are ( what ( make ( cream ( skimming work ) ) ) ) ) . ) )', 'hypothesis_parse': '(ROOT (S (NP (NN Product) (CC and) (NN geography)) (VP (VBP are) (SBAR (WHNP (WP what)) (S (VP (VBP make) (NP (NP (NN cream)) (VP (VBG skimming) (NP (NN work)))))))) (. .)))', 'genre': 'government', 'label': 1}\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"class MNLIDataset(Dataset):\n    def __init__(self, data, tokenizer, max_length):\n        self.data = data\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n    def __len__(self):\n        return len(self.data)\n    def  __getitem__(self, idx):\n        example = self.data[idx]\n        premise = example['premise']\n        hypothesis = example['hypothesis']\n        label = example['label']\n        encoded_pair = self.tokenizer.encode_plus(premise, hypothesis, max_length=self.max_length, padding='max_length', truncation=True,return_tensors='pt')\n        input_ids = encoded_pair['input_ids'].squeeze(0)\n        attention_mask = encoded_pair['attention_mask'].squeeze(0)\n        # Keeping this commented out for now, maybe not very essential for encoder only models? Investigate further...\n        # token_type_ids = encoded_pair.get('token_type_ids', torch.zeros_like(input_ids))\n        return {'input_ids': input_ids,'attention_mask': attention_mask,# 'token_type_ids': token_type_ids,\n'labels': torch.tensor(label)}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T11:04:18.905177Z","iopub.execute_input":"2025-03-29T11:04:18.905371Z","iopub.status.idle":"2025-03-29T11:04:18.919571Z","shell.execute_reply.started":"2025-03-29T11:04:18.905354Z","shell.execute_reply":"2025-03-29T11:04:18.918966Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"train_data = mnli_dataset[\"train\"]\nmax_seq_length = 128\ntrain_dataset = MNLIDataset(train_data, tokenizer, max_seq_length)\nprint(f\"Size of training dataset: {len(train_dataset)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T11:04:18.920708Z","iopub.execute_input":"2025-03-29T11:04:18.920982Z","iopub.status.idle":"2025-03-29T11:04:18.945430Z","shell.execute_reply.started":"2025-03-29T11:04:18.920947Z","shell.execute_reply":"2025-03-29T11:04:18.944632Z"}},"outputs":[{"name":"stdout","text":"Size of training dataset: 392702\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"sample = train_dataset[0]\nprint(sample['input_ids'].shape)\nprint(sample['attention_mask'].shape)\nprint(sample['labels'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T11:04:18.946274Z","iopub.execute_input":"2025-03-29T11:04:18.946533Z","iopub.status.idle":"2025-03-29T11:04:18.968461Z","shell.execute_reply.started":"2025-03-29T11:04:18.946495Z","shell.execute_reply":"2025-03-29T11:04:18.967536Z"}},"outputs":[{"name":"stdout","text":"torch.Size([128])\ntorch.Size([128])\ntensor(1)\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"class MultiHeadSelfAttention(nn.Module):\n    def __init__(self, hidden_size, num_attention_heads, dropout_rate):\n        super().__init__()\n        self.num_heads = num_attention_heads\n        self.head_dim = hidden_size // num_attention_heads\n        assert self.head_dim * self.num_heads == hidden_size\n\n        self.query = nn.Linear(hidden_size, hidden_size)\n        self.key = nn.Linear(hidden_size, hidden_size)\n        self.value = nn.Linear(hidden_size, hidden_size)\n\n        self.dropout = nn.Dropout(dropout_rate)\n        self.output = nn.Linear(hidden_size, hidden_size)\n    def forward(self, query, key, value, mask=None):\n        batch_size = query.size(0)\n        seq_len_q, seq_len_k, seq_len_v = query.size(1), key.size(1), value.size(1)\n        query = self.query(query)\n        key = self.key(key)\n        value = self.value(value)\n        query = query.view(batch_size, seq_len_q, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n        key = key.view(batch_size, seq_len_k, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n        value = value.view(batch_size, seq_len_v, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n        attention_scores = torch.matmul(query, key.transpose(-2,-1))\n        attention_scores = attention_scores/(self.head_dim**0.5)\n        mask = mask.unsqueeze(1).unsqueeze(2)\n        if mask is not None:\n            attention_scores = attention_scores.masked_fill(mask==0, float('-inf'))\n        attention_weights = F.softmax(attention_scores,dim=-1)\n        scaled_attention = torch.matmul(attention_weights, value)\n        scaled_attention = scaled_attention.permute(0, 2, 1, 3).contiguous().view(batch_size, seq_len_q, self.num_heads * self.head_dim)\n        output = self.output(scaled_attention)\n        return output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T11:04:18.969333Z","iopub.execute_input":"2025-03-29T11:04:18.969589Z","iopub.status.idle":"2025-03-29T11:04:18.985221Z","shell.execute_reply.started":"2025-03-29T11:04:18.969570Z","shell.execute_reply":"2025-03-29T11:04:18.984218Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"class FeedForwardNetwork(nn.Module):\n    def __init__(self, hidden_size, intermediate_size, dropout_rate):\n        super().__init__()\n        self.dense1 = nn.Linear(hidden_size, intermediate_size)\n        self.relu = nn.ReLU()\n        self.dense2 = nn.Linear(intermediate_size, hidden_size)\n        self.dropout = nn.Dropout(dropout_rate)\n\n    def forward(self, x):\n        x = F.relu(self.dense1(x))\n        x = self.dropout(x)\n        x = self.dense2(x)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T11:04:18.986348Z","iopub.execute_input":"2025-03-29T11:04:18.986621Z","iopub.status.idle":"2025-03-29T11:04:19.006230Z","shell.execute_reply.started":"2025-03-29T11:04:18.986588Z","shell.execute_reply":"2025-03-29T11:04:19.005268Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"class TransformerEncoderLayer(nn.Module):\n    def __init__(self, hidden_size, num_attention_heads, intermediate_size, dropout_rate):\n        super().__init__()\n        self.self_attention = MultiHeadSelfAttention(hidden_size, num_attention_heads, dropout_rate)\n        self.feed_forward = FeedForwardNetwork(hidden_size, intermediate_size, dropout_rate)\n        self.norm1 = nn.LayerNorm(hidden_size)\n        self.norm2 = nn.LayerNorm(hidden_size)\n        self.dropout = nn.Dropout(dropout_rate)\n\n    def forward(self, x, mask):\n        attention_output = self.self_attention(x,x, x, mask)\n        normed_output1 = self.norm1(attention_output + x)\n        ff_output = self.feed_forward(normed_output1)\n        final_output = self.norm2(ff_output + normed_output1)\n        return final_output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T11:04:19.007349Z","iopub.execute_input":"2025-03-29T11:04:19.007555Z","iopub.status.idle":"2025-03-29T11:04:19.025730Z","shell.execute_reply.started":"2025-03-29T11:04:19.007537Z","shell.execute_reply":"2025-03-29T11:04:19.024705Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"class FactorizedEmbedding(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_size, type_vocab_size=2):\n        super().__init__()\n        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n        self.token_type_embeddings = nn.Embedding(type_vocab_size, embedding_dim)\n        self.projection = nn.Linear(embedding_dim, hidden_size)\n\n    def forward(self, input_ids, token_type_ids=None):\n        factor_embeds = self.word_embeddings(input_ids)\n        if token_type_ids is None:\n            token_type_ids = torch.zeros_like(input_ids)\n        token_type_embeds = self.token_type_embeddings(token_type_ids)\n        # Sum the embeddings\n        embeds = factor_embeds + token_type_embeds        \n        project_embeds = self.projection(embeds)\n        return project_embeds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T11:04:19.026716Z","iopub.execute_input":"2025-03-29T11:04:19.027058Z","iopub.status.idle":"2025-03-29T11:04:19.039268Z","shell.execute_reply.started":"2025-03-29T11:04:19.027025Z","shell.execute_reply":"2025-03-29T11:04:19.038607Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"class PositionalEncoding(nn.Module):\n    def __init__(self, hidden_size, max_seq_length, dropout_rate):\n        super().__init__()\n        self.dropout = nn.Dropout(dropout_rate)\n\n        position = torch.arange(0, max_seq_length).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, hidden_size, 2) * -(math.log(10000.0) / hidden_size))\n        pe = torch.zeros(max_seq_length, 1, hidden_size)\n        pe[:, 0, 0::2] = torch.sin(position * div_term)\n        pe[:, 0, 1::2] = torch.cos(position * div_term)\n        self.register_buffer('pe', pe) # Store as a buffer (not a learnable parameter)\n\n    def forward(self, x):\n        seq_length = x.size(1)\n        pe = self.pe[:seq_length].squeeze(1)\n        x = x + pe.unsqueeze(0)\n        x = self.dropout(x)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T11:04:19.040020Z","iopub.execute_input":"2025-03-29T11:04:19.040267Z","iopub.status.idle":"2025-03-29T11:04:19.058625Z","shell.execute_reply.started":"2025-03-29T11:04:19.040247Z","shell.execute_reply":"2025-03-29T11:04:19.057855Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"class AtomBERT(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers, num_attention_heads, intermediate_size, num_classes, max_seq_length, dropout_rate, type_vocab_size=2):\n        super().__init__()\n        self.embedding = FactorizedEmbedding(vocab_size, embedding_dim, hidden_size, type_vocab_size)\n        self.positional_encoding = PositionalEncoding(hidden_size, max_seq_length, dropout_rate)\n        self.encoder_layer = TransformerEncoderLayer(hidden_size, num_attention_heads, intermediate_size, dropout_rate)\n        self.num_layers = num_layers\n        self.dropout = nn.Dropout(dropout_rate)\n        #self.classifier = nn.Linear(hidden_size, num_classes)\n        self.hidden_size = hidden_size\n\n    def forward(self, input_ids, attention_mask, token_type_ids=None):\n        embeddings = self.embedding(input_ids, token_type_ids)\n        embeddings = self.positional_encoding(embeddings)\n\n        # Share the encoder layer across all layers\n        encoder_output = embeddings\n        for _ in range(self.num_layers):\n            encoder_output = self.encoder_layer(encoder_output, attention_mask)\n\n        pooled_output = encoder_output[:, 0, :] # Shape: (batch_size, hidden_size)\n        pooled_output = self.dropout(pooled_output)\n        #logits = self.classifier(pooled_output)\n        return encoder_output, pooled_output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T11:04:19.059332Z","iopub.execute_input":"2025-03-29T11:04:19.059566Z","iopub.status.idle":"2025-03-29T11:04:19.074230Z","shell.execute_reply.started":"2025-03-29T11:04:19.059546Z","shell.execute_reply":"2025-03-29T11:04:19.073501Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"class AtomBERTForPretraining(nn.Module): # Renamed for clarity\n    def __init__(self, config):\n        super().__init__()\n        self.config = config # Store the config object\n        self.bert = AtomBERT(\n            vocab_size=config.vocab_size,\n            embedding_dim=config.embedding_dim,\n            hidden_size=config.hidden_size,\n            num_layers=config.num_layers,\n            num_attention_heads=config.num_attention_heads,\n            intermediate_size=config.intermediate_size,\n            num_classes=2, # For SOP\n            max_seq_length=config.max_seq_length,\n            dropout_rate=config.dropout_rate,\n            type_vocab_size=config.type_vocab_size if hasattr(config, 'type_vocab_size') else 2\n        )\n        self.mlm_head = nn.Sequential(\n        nn.Linear(config.hidden_size, config.hidden_size),\n        nn.LayerNorm(config.hidden_size),\n        nn.Linear(config.hidden_size, config.vocab_size)\n        )\n        self.sop_head = nn.Linear(config.hidden_size, 2)\n\n\n\n    def forward(self, input_ids, attention_mask, masked_labels=None, sentence_order_labels=None, token_type_ids=None):\n        outputs = self.bert(input_ids, attention_mask, token_type_ids) # Shape: (batch_size, seq_len, hidden_size)\n        encoder_output = outputs[0]\n        pooled_output = outputs[1]\n        # MLM Prediction\n        prediction_logits_mlm = self.mlm_head(encoder_output) # Shape: (batch_size, seq_len, vocab_size)\n\n        # SOP Prediction (using the pooled output - we might need to adjust this)\n        #pooled_output = outputs[:, 0, :] # Taking the [CLS] token representation\n        prediction_logits_sop = self.sop_head(pooled_output) # Shape: (batch_size, 2)\n\n        total_loss = None\n        if masked_labels is not None and sentence_order_labels is not None:\n            loss_fct_mlm = nn.CrossEntropyLoss()\n            masked_loss = loss_fct_mlm(prediction_logits_mlm.view(-1, self.config.vocab_size), masked_labels.view(-1))\n\n            loss_fct_sop = nn.CrossEntropyLoss()\n            sop_loss = loss_fct_sop(prediction_logits_sop.view(-1, 2), sentence_order_labels.view(-1))\n\n            total_loss = masked_loss + sop_loss # You might want to weigh these differently\n\n        elif masked_labels is not None:\n            loss_fct_mlm = nn.CrossEntropyLoss()\n            total_loss = loss_fct_mlm(prediction_logits_mlm.view(-1, self.config.vocab_size), masked_labels.view(-1))\n\n        elif sentence_order_labels is not None:\n            loss_fct_sop = nn.CrossEntropyLoss()\n            total_loss = loss_fct_sop(prediction_logits_sop.view(-1, 2), sentence_order_labels.view(-1))\n\n        return total_loss, prediction_logits_mlm, prediction_logits_sop","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T11:04:19.074820Z","iopub.execute_input":"2025-03-29T11:04:19.075064Z","iopub.status.idle":"2025-03-29T11:04:19.089948Z","shell.execute_reply.started":"2025-03-29T11:04:19.075044Z","shell.execute_reply":"2025-03-29T11:04:19.089328Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"class AtomBERTConfig:\n    def __init__(\n        self,\n        vocab_size,\n        embedding_dim=128,\n        hidden_size=768,\n        num_layers=4,\n        num_attention_heads=4,\n        intermediate_size=1500,\n        num_classes=2, # For SOP\n        max_seq_length=128,\n        dropout_rate=0.1,\n        type_vocab_size = 2\n    ):\n        self.vocab_size = vocab_size\n        self.embedding_dim = embedding_dim\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.num_attention_heads = num_attention_heads\n        self.intermediate_size = intermediate_size\n        self.num_classes = num_classes\n        self.max_seq_length = max_seq_length\n        self.dropout_rate = dropout_rate\n        self.type_vocab_size = type_vocab_size","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T11:04:19.090733Z","iopub.execute_input":"2025-03-29T11:04:19.090978Z","iopub.status.idle":"2025-03-29T11:04:19.108092Z","shell.execute_reply.started":"2025-03-29T11:04:19.090951Z","shell.execute_reply":"2025-03-29T11:04:19.107417Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"wiki_text_dataset = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\")\nprint(wiki_text_dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T11:04:19.108749Z","iopub.execute_input":"2025-03-29T11:04:19.108968Z","iopub.status.idle":"2025-03-29T11:04:21.358728Z","shell.execute_reply.started":"2025-03-29T11:04:19.108925Z","shell.execute_reply":"2025-03-29T11:04:21.357973Z"}},"outputs":[{"name":"stdout","text":"DatasetDict({\n    test: Dataset({\n        features: ['text'],\n        num_rows: 4358\n    })\n    train: Dataset({\n        features: ['text'],\n        num_rows: 1801350\n    })\n    validation: Dataset({\n        features: ['text'],\n        num_rows: 3760\n    })\n})\n","output_type":"stream"}],"execution_count":43},{"cell_type":"code","source":"vocab_size = tokenizer.vocab_size\nlearning_rate = 5e-5\n\n# Move the model to the device (GPU if available)\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n# Recall or re-initialize your tokenizer if needed\nvocab_size = tokenizer.vocab_size\n\n# Create the configuration\nconfig = AtomBERTConfig(vocab_size=vocab_size) # You can customize other parameters here if needed\n\n# Instantiate the AtomBERTForPretraining model\nmodelpretrain = AtomBERTForPretraining(config)\n\nprint(f\"Model instantiated with {config}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T11:04:21.361111Z","iopub.execute_input":"2025-03-29T11:04:21.361344Z","iopub.status.idle":"2025-03-29T11:04:21.647491Z","shell.execute_reply.started":"2025-03-29T11:04:21.361324Z","shell.execute_reply":"2025-03-29T11:04:21.646698Z"}},"outputs":[{"name":"stdout","text":"Model instantiated with <__main__.AtomBERTConfig object at 0x7c6735acfb20>\n","output_type":"stream"}],"execution_count":44},{"cell_type":"code","source":"def count_parameters(model):\n    total_params = 0\n    trainable_params = 0\n    non_trainable_params = 0\n\n    for name, param in model.named_parameters():\n        num_params = param.numel()\n        # if requires_grad is True, then it is a trainable parameter\n        if param.requires_grad:\n            trainable_params += num_params\n        else:\n            non_trainable_params += num_params\n        total_params += num_params\n\n    print(f\"Total Parameters: {total_params:,}\")\n    print(f\"Trainable Parameters: {trainable_params:,}\")\n    print(f\"Non-trainable Parameters: {non_trainable_params:,}\")\nprint(\"Basic Model: \\n\")\ncount_parameters(modelpretrain.bert)\nprint(\"Total Pretraining Model: \\n\")\ncount_parameters(modelpretrain)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T11:04:21.648643Z","iopub.execute_input":"2025-03-29T11:04:21.649012Z","iopub.status.idle":"2025-03-29T11:04:21.655981Z","shell.execute_reply.started":"2025-03-29T11:04:21.648977Z","shell.execute_reply":"2025-03-29T11:04:21.655308Z"}},"outputs":[{"name":"stdout","text":"Basic Model: \n\nTotal Parameters: 8,677,852\nTrainable Parameters: 8,677,852\nNon-trainable Parameters: 0\nTotal Pretraining Model: \n\nTotal Parameters: 32,742,936\nTrainable Parameters: 32,742,936\nNon-trainable Parameters: 0\n","output_type":"stream"}],"execution_count":45},{"cell_type":"code","source":"# Start a new wandb run to track this script.\nimport wandb\nrun = wandb.init(\n    # Set the wandb entity where your project will be logged (generally your team name).\n    entity=\"hexager-manipal\",\n    # Set the wandb project where this run will be logged.\n    project=\"MRM-transform\",\n    # Track hyperparameters and run metadata.\n    config={\n        \"learning_rate\": 0.02,\n        \"architecture\": \"AlBERT-32M\",\n        \"dataset\": \"WikiText-103\",\n        \"epochs\": 4,\n    },\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T11:04:21.656750Z","iopub.execute_input":"2025-03-29T11:04:21.657052Z","iopub.status.idle":"2025-03-29T11:04:28.008159Z","shell.execute_reply.started":"2025-03-29T11:04:21.657030Z","shell.execute_reply":"2025-03-29T11:04:28.007479Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250329_110421-66isnch9</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/hexager-manipal/MRM-transform/runs/66isnch9' target=\"_blank\">robust-wood-3</a></strong> to <a href='https://wandb.ai/hexager-manipal/MRM-transform' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/hexager-manipal/MRM-transform' target=\"_blank\">https://wandb.ai/hexager-manipal/MRM-transform</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/hexager-manipal/MRM-transform/runs/66isnch9' target=\"_blank\">https://wandb.ai/hexager-manipal/MRM-transform/runs/66isnch9</a>"},"metadata":{}}],"execution_count":46},{"cell_type":"code","source":"# Define parameters (ensure max_seq_length and batch_size are defined)\nmax_seq_length = 128  # Example value, adjust as needed\nbatch_size = 128      # Example value, adjust as needed\nnum_epochs = 4\nlearning_rate = 1e-4\nweight_decay = 0.01\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n\n# Load tokenizer\ntokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n\n# Load dataset\nwiki_text_dataset = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\")\n\n# Tokenize dataset\ndef tokenize_function(examples):\n    return tokenizer(examples[\"text\"], truncation=True, padding='max_length', max_length=max_seq_length)\ntokenized_train_dataset = wiki_text_dataset[\"train\"].map(tokenize_function, batched=True)\n\n# Prepare MLM data\ndef prepare_mlm_data(examples):\n    input_ids = torch.tensor(examples[\"input_ids\"])\n    batch_size_local = len(input_ids)\n    sequence_length = len(input_ids[0])\n    mask_prob = 0.15\n    mask_token_id = tokenizer.mask_token_id\n    pad_token_id = tokenizer.pad_token_id\n    cls_token_id = tokenizer.cls_token_id\n    sep_token_id = tokenizer.sep_token_id\n\n    rand = torch.rand(batch_size_local, sequence_length)\n    mask = (rand < mask_prob) & (input_ids != cls_token_id) & (input_ids != sep_token_id) & (input_ids != pad_token_id)\n\n    labels = input_ids.clone()\n    labels[~mask] = -100\n    inputs = input_ids.clone()\n    inputs[mask] = mask_token_id\n\n    return {\"input_ids\": inputs, \"labels\": labels}\nmlm_tokenized_train_dataset = tokenized_train_dataset.map(\n    prepare_mlm_data,\n    batched=True,\n    remove_columns=[\"text\", \"token_type_ids\"]\n)\n\n# Prepare SOP data\nimport re\nfrom datasets import Dataset as HFDataset\n\ndef split_into_sentences_robust(text):\n    sentence_pattern = r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?|!)\\s+'\n    sentences = re.split(sentence_pattern, text)\n    return [s.strip() for s in sentences if s.strip()]\n\nsop_texts_limited = []\nsop_labels_limited = []\ncls_token = tokenizer.cls_token\nsep_token = tokenizer.sep_token\nmax_pairs_per_document = 5\n\nfor example in wiki_text_dataset[\"train\"]:\n    text = example[\"text\"]\n    sentences = split_into_sentences_robust(text)\n    pairs_count = 0\n    for i in range(len(sentences) - 1):\n        if pairs_count >= max_pairs_per_document:\n            break\n        sentence_a = sentences[i]\n        sentence_b = sentences[i + 1]\n        sop_texts_limited.append(cls_token + \" \" + sentence_a + \" \" + sep_token + \" \" + sentence_b)\n        sop_labels_limited.append(0)\n        pairs_count += 1\n        if pairs_count >= max_pairs_per_document:\n            break\n        sop_texts_limited.append(cls_token + \" \" + sentence_b + \" \" + sep_token + \" \" + sentence_a)\n        sop_labels_limited.append(1)\n        pairs_count += 1\n\nsop_dataset = HFDataset.from_dict({\"text\": sop_texts_limited, \"sentence_order_labels\": sop_labels_limited})\n\ndef tokenize_sop_function(examples):\n    return tokenizer(examples[\"text\"], truncation=True, padding='max_length', max_length=max_seq_length)\ntokenized_sop_dataset = sop_dataset.map(tokenize_sop_function, batched=True)\n\n# Set format to PyTorch tensors\nmlm_tokenized_train_dataset.set_format(\"torch\")\ntokenized_sop_dataset.set_format(\"torch\")\n\n# DataLoader for MLM dataset\nmlm_dataloader = DataLoader(\n    mlm_tokenized_train_dataset,\n    batch_size=batch_size,\n    shuffle=True\n)\n\n# DataLoader for SOP dataset\nsop_dataloader = DataLoader(\n    tokenized_sop_dataset,\n    batch_size=batch_size,\n    shuffle=True\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T11:04:28.008873Z","iopub.execute_input":"2025-03-29T11:04:28.009091Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1801350 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bac4218536c0431bb8a37e6c8d0f52f9"}},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"modelpretrain.to(device)\nfrom torch.optim import AdamW\noptimizer = AdamW(modelpretrain.parameters(), lr=learning_rate, weight_decay=weight_decay)\nwandb.watch(modelpretrain)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for epoch in range(num_epochs):\n    modelpretrain.train()\n    total_mlm_loss = 0\n    total_sop_loss = 0\n    total_steps = min(len(mlm_dataloader), len(sop_dataloader))\n\n    progress_bar = tqdm(range(total_steps), desc=f\"Epoch {epoch+1}/{num_epochs}\")\n\n    for step in progress_bar:\n        try:\n            batch_mlm = next(iter(mlm_dataloader))\n        except StopIteration:\n            print(\"MLM dataloader finished early.\")\n            break\n        try:\n            batch_sop = next(iter(sop_dataloader))\n        except StopIteration:\n            print(\"SOP dataloader finished early.\")\n            break\n\n        # Move MLM batch to device\n        mlm_inputs = {k: batch_mlm[k].to(device) for k in batch_mlm}\n\n        # Move SOP batch to device\n        sop_inputs = {\n                    k: v.to(device) if isinstance(v, torch.Tensor) else v\n                    for k, v in batch_sop.items()\n                    }\n\n        # Zero gradients\n        optimizer.zero_grad()\n\n        # MLM forward pass\n        returned_loss, prediction_logits_mlm, prediction_logits_sop = modelpretrain(\n            input_ids=mlm_inputs['input_ids'].to(device),\n            attention_mask=mlm_inputs['attention_mask'].to(device),\n            masked_labels=mlm_inputs['labels'].to(device)\n        )\n\n        mlm_loss = returned_loss # Now you can use 'returned_loss' which holds the total loss\n\n        # SOP forward pass\n        outputs_sop = modelpretrain(**{\n            'input_ids': sop_inputs['input_ids'].to(device),\n            'attention_mask': sop_inputs['attention_mask'].to(device),\n            'token_type_ids' : sop_inputs.get('token_type_ids', torch.tensor([], device=device)).to(device),\n            'sentence_order_labels': sop_inputs['sentence_order_labels'].to(device)\n        })\n        sop_loss = outputs_sop[0]\n\n        # Combine losses\n        total_loss = mlm_loss + sop_loss\n\n        # Backward pass\n        total_loss.backward()\n\n        # Update parameters\n        optimizer.step()\n\n        total_mlm_loss += mlm_loss.item()\n        total_sop_loss += sop_loss.item()\n        wandb.log({\"mlm_loss\": mlm_loss.item(), \"sop_loss\": sop_loss.item(), \"total_loss\": total_loss.item()})\n        progress_bar.set_postfix(mlm_loss=f\"{mlm_loss.item():.4f}\", sop_loss=f\"{sop_loss.item():.4f}\")\n        \n    avg_mlm_loss = total_mlm_loss / total_steps if total_steps > 0 else 0\n    avg_sop_loss = total_sop_loss / total_steps if total_steps > 0 else 0\n    run.log({\"Total Unsupervised Loss\": total_loss})\n    save_path = '/kaggle/working/pretrained_model.pth'  # Replace with your desired path\n    wandb.log({\"avg_mlm_loss\": avg_mlm_loss, \"avg_sop_loss\": avg_sop_loss, \"epoch\": epoch + 1})\n    # Save the state_dict of the model\n    torch.save(modelpretrain.state_dict(), save_path)\n    wandb.save(save_path)\n    print(f\"Pretrained model weights saved to: {save_path}\")\n    print(f\"Epoch {epoch+1}/{num_epochs} finished, Average MLM Loss: {avg_mlm_loss:.4f}, Average SOP Loss: {avg_sop_loss:.4f}\")\n\nprint(\"Pretraining finished!\")\nrun.finish()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class AtomBERTForSequenceClassification(nn.Module):\n    def __init__(self, pretrained_config, num_classes):\n        super().__init__()\n        self.bert = AtomBERT(\n            vocab_size=pretrained_config.vocab_size,\n            embedding_dim=pretrained_config.embedding_dim,\n            hidden_size=pretrained_config.hidden_size,\n            num_layers=pretrained_config.num_layers,\n            num_attention_heads=pretrained_config.num_attention_heads,\n            intermediate_size=pretrained_config.intermediate_size,\n            num_classes=num_classes, # For MNLI this will be 3\n            max_seq_length=pretrained_config.max_seq_length,\n            dropout_rate=pretrained_config.dropout_rate,\n            type_vocab_size=pretrained_config.type_vocab_size if hasattr(pretrained_config, 'type_vocab_size') else 2\n        )\n        self.dropout = nn.Dropout(pretrained_config.dropout_rate)\n        self.classifier = nn.Linear(pretrained_config.hidden_size, num_classes)\n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.bert(input_ids, attention_mask)\n        pooled_output = outputs[1] # We typically use the pooled output for classification\n        pooled_output = self.dropout(pooled_output)\n        logits = self.classifier(pooled_output)\n        return logits","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n# Define the path to your saved pretrained weights file\npretrained_weights_path = '/kaggle/working/pretrained_model.pth' # Replace with the actual path to your saved file\n\n# Load the configuration of your pretrained model\n# Make sure these values match what you used for pretraining\npretrained_config = AtomBERTConfig(\n    vocab_size=tokenizer.vocab_size,\n    embedding_dim=128,\n    hidden_size=768,\n    num_layers=4,\n    num_attention_heads=4,\n    intermediate_size=1500,\n    max_seq_length=128,\n    dropout_rate=0.1,\n    type_vocab_size=2\n)\n\n# Instantiate the model for sequence classification (for MNLI with 3 classes)\nnum_classes_mnli = 3\nmodel_for_mnli = AtomBERTForSequenceClassification(pretrained_config, num_classes_mnli)\n\n# Load the state dictionary of the pretrained model\npretrained_state_dict = torch.load(pretrained_weights_path, map_location=torch.device(device)) # Ensure device consistency\n\n# Create a new dictionary to store only the 'bert' weights\npretrained_bert_state_dict = {}\nfor name, param in pretrained_state_dict.items():\n    if name.startswith('bert.'):\n        pretrained_bert_state_dict[name[len('bert.'):]] = param\n\n# Load the pretrained weights into the 'bert' part of the sequence classification model\nmodel_for_mnli.bert.load_state_dict(pretrained_bert_state_dict)\n\nprint(\"Pretrained weights loaded successfully into the sequence classification model.\")\n\n# Move the fine-tuning model to the device\nmodel_for_mnli.to(device)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}