{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-29T11:30:51.329912Z","iopub.execute_input":"2025-03-29T11:30:51.330268Z","iopub.status.idle":"2025-03-29T11:30:51.635456Z","shell.execute_reply.started":"2025-03-29T11:30:51.330242Z","shell.execute_reply":"2025-03-29T11:30:51.634629Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"from transformers import BertTokenizerFast\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import load_dataset\nfrom datasets import Dataset as HFDataset\nimport torch\nimport torch.nn.functional as F\nimport math\nimport torch.nn as nn\nimport torch.optim as optim\nfrom tqdm.notebook import tqdm\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"wandbpass\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T11:30:51.636758Z","iopub.execute_input":"2025-03-29T11:30:51.637283Z","iopub.status.idle":"2025-03-29T11:30:57.679490Z","shell.execute_reply.started":"2025-03-29T11:30:51.637244Z","shell.execute_reply":"2025-03-29T11:30:57.678834Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import wandb\nwandb.login(key=secret_value_0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T11:30:57.680793Z","iopub.execute_input":"2025-03-29T11:30:57.681264Z","iopub.status.idle":"2025-03-29T11:31:06.212402Z","shell.execute_reply.started":"2025-03-29T11:30:57.681241Z","shell.execute_reply":"2025-03-29T11:31:06.211633Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhexager\u001b[0m (\u001b[33mhexager-manipal\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\nsentence = \"By the way, we ball\"\ntokens = tokenizer.tokenize(sentence)\nprint(tokens)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T11:31:06.213615Z","iopub.execute_input":"2025-03-29T11:31:06.214128Z","iopub.status.idle":"2025-03-29T11:31:10.966864Z","shell.execute_reply.started":"2025-03-29T11:31:06.214105Z","shell.execute_reply":"2025-03-29T11:31:10.966068Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b97518d23a74ccfae91165ec6d12556"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e0a1412a53fb4a948f3f6d615c19ef25"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b5eb7545136c4bdd886b6315413c25b0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a59e460106524e88a7fa8cdbccb2fdb4"}},"metadata":{}},{"name":"stdout","text":"['by', 'the', 'way', ',', 'we', 'ball']\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"mnli_dataset = load_dataset(\"multi_nli\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T11:31:10.967722Z","iopub.execute_input":"2025-03-29T11:31:10.968307Z","iopub.status.idle":"2025-03-29T11:31:18.402579Z","shell.execute_reply.started":"2025-03-29T11:31:10.968280Z","shell.execute_reply":"2025-03-29T11:31:18.401912Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/8.89k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c6daf1aea45462c8495d411cd8f0a26"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/214M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d21bf860f76490d935c3f4e3775c327"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(…)alidation_matched-00000-of-00001.parquet:   0%|          | 0.00/4.94M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"68e46a25421d4c3990a4693c998054b3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(…)dation_mismatched-00000-of-00001.parquet:   0%|          | 0.00/5.10M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"85745ac24dab42e58a11a214ac2d1f63"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/392702 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d26f55713614626b6acdba47b86d22b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation_matched split:   0%|          | 0/9815 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c0d4b3a299d4da3a5890c2b8a6c61ae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation_mismatched split:   0%|          | 0/9832 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d5ce809236e4a6185bfaa3d8d2cc570"}},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"val_data = mnli_dataset[\"validation_matched\"] # You can also try \"validation_mismatched\"\ntrain_data = mnli_dataset[\"train\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T13:12:01.959290Z","iopub.execute_input":"2025-03-29T13:12:01.959594Z","iopub.status.idle":"2025-03-29T13:12:01.964072Z","shell.execute_reply.started":"2025-03-29T13:12:01.959571Z","shell.execute_reply":"2025-03-29T13:12:01.963222Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"print(mnli_dataset['train'][0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T11:31:18.403317Z","iopub.execute_input":"2025-03-29T11:31:18.403539Z","iopub.status.idle":"2025-03-29T11:31:18.408528Z","shell.execute_reply.started":"2025-03-29T11:31:18.403519Z","shell.execute_reply":"2025-03-29T11:31:18.407598Z"}},"outputs":[{"name":"stdout","text":"{'promptID': 31193, 'pairID': '31193n', 'premise': 'Conceptually cream skimming has two basic dimensions - product and geography.', 'premise_binary_parse': '( ( Conceptually ( cream skimming ) ) ( ( has ( ( ( two ( basic dimensions ) ) - ) ( ( product and ) geography ) ) ) . ) )', 'premise_parse': '(ROOT (S (NP (JJ Conceptually) (NN cream) (NN skimming)) (VP (VBZ has) (NP (NP (CD two) (JJ basic) (NNS dimensions)) (: -) (NP (NN product) (CC and) (NN geography)))) (. .)))', 'hypothesis': 'Product and geography are what make cream skimming work. ', 'hypothesis_binary_parse': '( ( ( Product and ) geography ) ( ( are ( what ( make ( cream ( skimming work ) ) ) ) ) . ) )', 'hypothesis_parse': '(ROOT (S (NP (NN Product) (CC and) (NN geography)) (VP (VBP are) (SBAR (WHNP (WP what)) (S (VP (VBP make) (NP (NP (NN cream)) (VP (VBG skimming) (NP (NN work)))))))) (. .)))', 'genre': 'government', 'label': 1}\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"class MNLIDataset(Dataset):\n    def __init__(self, data, tokenizer, max_length):\n        self.data = data\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n    def __len__(self):\n        return len(self.data)\n    def  __getitem__(self, idx):\n        example = self.data[idx]\n        premise = example['premise']\n        hypothesis = example['hypothesis']\n        label = example['label']\n        encoded_pair = self.tokenizer.encode_plus(premise, hypothesis, max_length=self.max_length, padding='max_length', truncation=True,return_tensors='pt')\n        input_ids = encoded_pair['input_ids'].squeeze(0)\n        attention_mask = encoded_pair['attention_mask'].squeeze(0)\n        # Keeping this commented out for now, maybe not very essential for encoder only models? Investigate further...\n        # token_type_ids = encoded_pair.get('token_type_ids', torch.zeros_like(input_ids))\n        return {'input_ids': input_ids,'attention_mask': attention_mask,# 'token_type_ids': token_type_ids,\n'labels': torch.tensor(label)}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T11:31:18.409635Z","iopub.execute_input":"2025-03-29T11:31:18.409969Z","iopub.status.idle":"2025-03-29T11:31:18.615284Z","shell.execute_reply.started":"2025-03-29T11:31:18.409904Z","shell.execute_reply":"2025-03-29T11:31:18.614387Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"train_data = mnli_dataset[\"train\"]\nmax_seq_length = 128\ntrain_dataset = MNLIDataset(train_data, tokenizer, max_seq_length)\nval_dataset = MNLIDataset(val_data, tokenizer, max_seq_length)\nprint(f\"Size of training dataset: {len(train_dataset)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T13:12:08.563505Z","iopub.execute_input":"2025-03-29T13:12:08.563802Z","iopub.status.idle":"2025-03-29T13:12:08.575701Z","shell.execute_reply.started":"2025-03-29T13:12:08.563773Z","shell.execute_reply":"2025-03-29T13:12:08.574729Z"}},"outputs":[{"name":"stdout","text":"Size of training dataset: 392702\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"batch_size_finetune = 32 # You can adjust this\ntrain_dataloader = DataLoader(train_dataset, batch_size=batch_size_finetune, shuffle=True)\nval_dataloader = DataLoader(val_dataset, batch_size=batch_size_finetune)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T13:12:10.918118Z","iopub.execute_input":"2025-03-29T13:12:10.918395Z","iopub.status.idle":"2025-03-29T13:12:10.923420Z","shell.execute_reply.started":"2025-03-29T13:12:10.918374Z","shell.execute_reply":"2025-03-29T13:12:10.922480Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"sample = train_dataset[0]\nprint(sample['input_ids'].shape)\nprint(sample['attention_mask'].shape)\nprint(sample['labels'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T11:31:18.634490Z","iopub.execute_input":"2025-03-29T11:31:18.634694Z","iopub.status.idle":"2025-03-29T11:31:18.685683Z","shell.execute_reply.started":"2025-03-29T11:31:18.634676Z","shell.execute_reply":"2025-03-29T11:31:18.685074Z"}},"outputs":[{"name":"stdout","text":"torch.Size([128])\ntorch.Size([128])\ntensor(1)\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"class MultiHeadSelfAttention(nn.Module):\n    def __init__(self, hidden_size, num_attention_heads, dropout_rate):\n        super().__init__()\n        self.num_heads = num_attention_heads\n        self.head_dim = hidden_size // num_attention_heads\n        assert self.head_dim * self.num_heads == hidden_size\n\n        self.query = nn.Linear(hidden_size, hidden_size)\n        self.key = nn.Linear(hidden_size, hidden_size)\n        self.value = nn.Linear(hidden_size, hidden_size)\n\n        self.dropout = nn.Dropout(dropout_rate)\n        self.output = nn.Linear(hidden_size, hidden_size)\n    def forward(self, query, key, value, mask=None):\n        batch_size = query.size(0)\n        seq_len_q, seq_len_k, seq_len_v = query.size(1), key.size(1), value.size(1)\n        query = self.query(query)\n        key = self.key(key)\n        value = self.value(value)\n        query = query.view(batch_size, seq_len_q, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n        key = key.view(batch_size, seq_len_k, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n        value = value.view(batch_size, seq_len_v, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n        attention_scores = torch.matmul(query, key.transpose(-2,-1))\n        attention_scores = attention_scores/(self.head_dim**0.5)\n        mask = mask.unsqueeze(1).unsqueeze(2)\n        if mask is not None:\n            attention_scores = attention_scores.masked_fill(mask==0, float('-inf'))\n        attention_weights = F.softmax(attention_scores,dim=-1)\n        scaled_attention = torch.matmul(attention_weights, value)\n        scaled_attention = scaled_attention.permute(0, 2, 1, 3).contiguous().view(batch_size, seq_len_q, self.num_heads * self.head_dim)\n        output = self.output(scaled_attention)\n        return output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T11:31:18.686423Z","iopub.execute_input":"2025-03-29T11:31:18.686674Z","iopub.status.idle":"2025-03-29T11:31:18.694322Z","shell.execute_reply.started":"2025-03-29T11:31:18.686654Z","shell.execute_reply":"2025-03-29T11:31:18.693469Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"class FeedForwardNetwork(nn.Module):\n    def __init__(self, hidden_size, intermediate_size, dropout_rate):\n        super().__init__()\n        self.dense1 = nn.Linear(hidden_size, intermediate_size)\n        self.relu = nn.ReLU()\n        self.dense2 = nn.Linear(intermediate_size, hidden_size)\n        self.dropout = nn.Dropout(dropout_rate)\n\n    def forward(self, x):\n        x = F.relu(self.dense1(x))\n        x = self.dropout(x)\n        x = self.dense2(x)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T11:31:18.695189Z","iopub.execute_input":"2025-03-29T11:31:18.695418Z","iopub.status.idle":"2025-03-29T11:31:18.714145Z","shell.execute_reply.started":"2025-03-29T11:31:18.695387Z","shell.execute_reply":"2025-03-29T11:31:18.713422Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"class TransformerEncoderLayer(nn.Module):\n    def __init__(self, hidden_size, num_attention_heads, intermediate_size, dropout_rate):\n        super().__init__()\n        self.self_attention = MultiHeadSelfAttention(hidden_size, num_attention_heads, dropout_rate)\n        self.feed_forward = FeedForwardNetwork(hidden_size, intermediate_size, dropout_rate)\n        self.norm1 = nn.LayerNorm(hidden_size)\n        self.norm2 = nn.LayerNorm(hidden_size)\n        self.dropout = nn.Dropout(dropout_rate)\n\n    def forward(self, x, mask):\n        attention_output = self.self_attention(x,x, x, mask)\n        normed_output1 = self.norm1(attention_output + x)\n        ff_output = self.feed_forward(normed_output1)\n        final_output = self.norm2(ff_output + normed_output1)\n        return final_output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T11:31:18.715023Z","iopub.execute_input":"2025-03-29T11:31:18.715301Z","iopub.status.idle":"2025-03-29T11:31:18.731288Z","shell.execute_reply.started":"2025-03-29T11:31:18.715273Z","shell.execute_reply":"2025-03-29T11:31:18.730653Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"class FactorizedEmbedding(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_size, type_vocab_size=2):\n        super().__init__()\n        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n        self.token_type_embeddings = nn.Embedding(type_vocab_size, embedding_dim)\n        self.projection = nn.Linear(embedding_dim, hidden_size)\n\n    def forward(self, input_ids, token_type_ids=None):\n        factor_embeds = self.word_embeddings(input_ids)\n        if token_type_ids is None:\n            token_type_ids = torch.zeros_like(input_ids)\n        token_type_embeds = self.token_type_embeddings(token_type_ids)\n        # Sum the embeddings\n        embeds = factor_embeds + token_type_embeds        \n        project_embeds = self.projection(embeds)\n        return project_embeds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T11:31:18.732080Z","iopub.execute_input":"2025-03-29T11:31:18.732343Z","iopub.status.idle":"2025-03-29T11:31:18.745178Z","shell.execute_reply.started":"2025-03-29T11:31:18.732317Z","shell.execute_reply":"2025-03-29T11:31:18.744403Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"class PositionalEncoding(nn.Module):\n    def __init__(self, hidden_size, max_seq_length, dropout_rate):\n        super().__init__()\n        self.dropout = nn.Dropout(dropout_rate)\n\n        position = torch.arange(0, max_seq_length).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, hidden_size, 2) * -(math.log(10000.0) / hidden_size))\n        pe = torch.zeros(max_seq_length, 1, hidden_size)\n        pe[:, 0, 0::2] = torch.sin(position * div_term)\n        pe[:, 0, 1::2] = torch.cos(position * div_term)\n        self.register_buffer('pe', pe) # Store as a buffer (not a learnable parameter)\n\n    def forward(self, x):\n        seq_length = x.size(1)\n        pe = self.pe[:seq_length].squeeze(1)\n        x = x + pe.unsqueeze(0)\n        x = self.dropout(x)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T11:31:18.746048Z","iopub.execute_input":"2025-03-29T11:31:18.746322Z","iopub.status.idle":"2025-03-29T11:31:18.762731Z","shell.execute_reply.started":"2025-03-29T11:31:18.746296Z","shell.execute_reply":"2025-03-29T11:31:18.761891Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"class AtomBERT(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers, num_attention_heads, intermediate_size, num_classes, max_seq_length, dropout_rate, type_vocab_size=2):\n        super().__init__()\n        self.embedding = FactorizedEmbedding(vocab_size, embedding_dim, hidden_size, type_vocab_size)\n        self.positional_encoding = PositionalEncoding(hidden_size, max_seq_length, dropout_rate)\n        self.encoder_layer = TransformerEncoderLayer(hidden_size, num_attention_heads, intermediate_size, dropout_rate)\n        self.num_layers = num_layers\n        self.dropout = nn.Dropout(dropout_rate)\n        #self.classifier = nn.Linear(hidden_size, num_classes)\n        self.hidden_size = hidden_size\n\n    def forward(self, input_ids, attention_mask, token_type_ids=None):\n        embeddings = self.embedding(input_ids, token_type_ids)\n        embeddings = self.positional_encoding(embeddings)\n\n        # Share the encoder layer across all layers\n        encoder_output = embeddings\n        for _ in range(self.num_layers):\n            encoder_output = self.encoder_layer(encoder_output, attention_mask)\n\n        pooled_output = encoder_output[:, 0, :] # Shape: (batch_size, hidden_size)\n        pooled_output = self.dropout(pooled_output)\n        #logits = self.classifier(pooled_output)\n        return encoder_output, pooled_output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T11:31:18.763545Z","iopub.execute_input":"2025-03-29T11:31:18.763815Z","iopub.status.idle":"2025-03-29T11:31:18.776716Z","shell.execute_reply.started":"2025-03-29T11:31:18.763787Z","shell.execute_reply":"2025-03-29T11:31:18.775996Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"class AtomBERTForPretraining(nn.Module): # Renamed for clarity\n    def __init__(self, config):\n        super().__init__()\n        self.config = config # Store the config object\n        self.bert = AtomBERT(\n            vocab_size=config.vocab_size,\n            embedding_dim=config.embedding_dim,\n            hidden_size=config.hidden_size,\n            num_layers=config.num_layers,\n            num_attention_heads=config.num_attention_heads,\n            intermediate_size=config.intermediate_size,\n            num_classes=2, # For SOP\n            max_seq_length=config.max_seq_length,\n            dropout_rate=config.dropout_rate,\n            type_vocab_size=config.type_vocab_size if hasattr(config, 'type_vocab_size') else 2\n        )\n        self.mlm_head = nn.Sequential(\n        nn.Linear(config.hidden_size, config.hidden_size),\n        nn.LayerNorm(config.hidden_size),\n        nn.Linear(config.hidden_size, config.vocab_size)\n        )\n        self.sop_head = nn.Linear(config.hidden_size, 2)\n\n\n\n    def forward(self, input_ids, attention_mask, masked_labels=None, sentence_order_labels=None, token_type_ids=None):\n        outputs = self.bert(input_ids, attention_mask, token_type_ids) # Shape: (batch_size, seq_len, hidden_size)\n        encoder_output = outputs[0]\n        pooled_output = outputs[1]\n        # MLM Prediction\n        prediction_logits_mlm = self.mlm_head(encoder_output) # Shape: (batch_size, seq_len, vocab_size)\n\n        # SOP Prediction (using the pooled output - we might need to adjust this)\n        #pooled_output = outputs[:, 0, :] # Taking the [CLS] token representation\n        prediction_logits_sop = self.sop_head(pooled_output) # Shape: (batch_size, 2)\n\n        total_loss = None\n        if masked_labels is not None and sentence_order_labels is not None:\n            loss_fct_mlm = nn.CrossEntropyLoss()\n            masked_loss = loss_fct_mlm(prediction_logits_mlm.view(-1, self.config.vocab_size), masked_labels.view(-1))\n\n            loss_fct_sop = nn.CrossEntropyLoss()\n            sop_loss = loss_fct_sop(prediction_logits_sop.view(-1, 2), sentence_order_labels.view(-1))\n\n            total_loss = masked_loss + sop_loss # You might want to weigh these differently\n\n        elif masked_labels is not None:\n            loss_fct_mlm = nn.CrossEntropyLoss()\n            total_loss = loss_fct_mlm(prediction_logits_mlm.view(-1, self.config.vocab_size), masked_labels.view(-1))\n\n        elif sentence_order_labels is not None:\n            loss_fct_sop = nn.CrossEntropyLoss()\n            total_loss = loss_fct_sop(prediction_logits_sop.view(-1, 2), sentence_order_labels.view(-1))\n\n        return total_loss, prediction_logits_mlm, prediction_logits_sop","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T11:31:18.777557Z","iopub.execute_input":"2025-03-29T11:31:18.777829Z","iopub.status.idle":"2025-03-29T11:31:18.794816Z","shell.execute_reply.started":"2025-03-29T11:31:18.777804Z","shell.execute_reply":"2025-03-29T11:31:18.794053Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"class AtomBERTConfig:\n    def __init__(\n        self,\n        vocab_size,\n        embedding_dim=128,\n        hidden_size=768,\n        num_layers=4,\n        num_attention_heads=4,\n        intermediate_size=1500,\n        num_classes=2, # For SOP\n        max_seq_length=128,\n        dropout_rate=0.1,\n        type_vocab_size = 2\n    ):\n        self.vocab_size = vocab_size\n        self.embedding_dim = embedding_dim\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.num_attention_heads = num_attention_heads\n        self.intermediate_size = intermediate_size\n        self.num_classes = num_classes\n        self.max_seq_length = max_seq_length\n        self.dropout_rate = dropout_rate\n        self.type_vocab_size = type_vocab_size","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T13:32:51.588356Z","iopub.execute_input":"2025-03-29T13:32:51.588651Z","iopub.status.idle":"2025-03-29T13:32:51.594526Z","shell.execute_reply.started":"2025-03-29T13:32:51.588626Z","shell.execute_reply":"2025-03-29T13:32:51.593613Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"wiki_text_dataset = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\")\nprint(wiki_text_dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T11:31:18.812339Z","iopub.execute_input":"2025-03-29T11:31:18.812531Z","iopub.status.idle":"2025-03-29T11:31:27.513208Z","shell.execute_reply.started":"2025-03-29T11:31:18.812515Z","shell.execute_reply":"2025-03-29T11:31:27.512529Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/10.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2369d387c1004709baf57073c67a3102"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/733k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"67834065b79e47318b547b140df5c0b9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00002.parquet:   0%|          | 0.00/157M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a83cd20a7934f439857c2d8497cdb29"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00001-of-00002.parquet:   0%|          | 0.00/157M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab7c6023e620491cb7e236e866a155c5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/657k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7c145f0f41e4ced8a32cd58f7efd67c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa49ed49974d47a789bb8b737f224e0b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/1801350 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b20f8a7a3121477e976fb15ec8b895a2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e039e4f22c5f44ec8b48265d38e491b8"}},"metadata":{}},{"name":"stdout","text":"DatasetDict({\n    test: Dataset({\n        features: ['text'],\n        num_rows: 4358\n    })\n    train: Dataset({\n        features: ['text'],\n        num_rows: 1801350\n    })\n    validation: Dataset({\n        features: ['text'],\n        num_rows: 3760\n    })\n})\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"vocab_size = tokenizer.vocab_size\nlearning_rate = 5e-5\n\n# Move the model to the device (GPU if available)\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n# Recall or re-initialize your tokenizer if needed\nvocab_size = tokenizer.vocab_size\n\n# Create the configuration\nconfig = AtomBERTConfig(vocab_size=vocab_size) # You can customize other parameters here if needed\n\n# Instantiate the AtomBERTForPretraining model\nmodelpretrain = AtomBERTForPretraining(config)\n\nprint(f\"Model instantiated with {config}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T13:32:59.865413Z","iopub.execute_input":"2025-03-29T13:32:59.865724Z","iopub.status.idle":"2025-03-29T13:33:00.138239Z","shell.execute_reply.started":"2025-03-29T13:32:59.865697Z","shell.execute_reply":"2025-03-29T13:33:00.137303Z"}},"outputs":[{"name":"stdout","text":"Model instantiated with <__main__.AtomBERTConfig object at 0x7a3e3da9ce20>\n","output_type":"stream"}],"execution_count":49},{"cell_type":"code","source":"def count_parameters(model):\n    total_params = 0\n    trainable_params = 0\n    non_trainable_params = 0\n\n    for name, param in model.named_parameters():\n        num_params = param.numel()\n        # if requires_grad is True, then it is a trainable parameter\n        if param.requires_grad:\n            trainable_params += num_params\n        else:\n            non_trainable_params += num_params\n        total_params += num_params\n\n    print(f\"Total Parameters: {total_params:,}\")\n    print(f\"Trainable Parameters: {trainable_params:,}\")\n    print(f\"Non-trainable Parameters: {non_trainable_params:,}\")\nprint(\"Basic Model: \\n\")\ncount_parameters(modelpretrain.bert)\nprint(\"Total Pretraining Model: \\n\")\ncount_parameters(modelpretrain)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T13:33:04.989761Z","iopub.execute_input":"2025-03-29T13:33:04.990093Z","iopub.status.idle":"2025-03-29T13:33:05.001394Z","shell.execute_reply.started":"2025-03-29T13:33:04.990067Z","shell.execute_reply":"2025-03-29T13:33:05.000473Z"}},"outputs":[{"name":"stdout","text":"Basic Model: \n\nTotal Parameters: 8,677,852\nTrainable Parameters: 8,677,852\nNon-trainable Parameters: 0\nTotal Pretraining Model: \n\nTotal Parameters: 32,742,936\nTrainable Parameters: 32,742,936\nNon-trainable Parameters: 0\n","output_type":"stream"}],"execution_count":50},{"cell_type":"code","source":"# Start a new wandb run to track this script.\nimport wandb\nrun = wandb.init(\n    # Set the wandb entity where your project will be logged (generally your team name).\n    entity=\"hexager-manipal\",\n    # Set the wandb project where this run will be logged.\n    project=\"MRM-transform\",\n    # Track hyperparameters and run metadata.\n    config={\n        \"learning_rate\": 0.02,\n        \"architecture\": \"AlBERT-1M\",\n        \"dataset\": \"WikiText-103\",\n        \"epochs\": 4,\n    },\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T13:20:38.802065Z","iopub.execute_input":"2025-03-29T13:20:38.802354Z","iopub.status.idle":"2025-03-29T13:20:38.817686Z","shell.execute_reply.started":"2025-03-29T13:20:38.802331Z","shell.execute_reply":"2025-03-29T13:20:38.817020Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"# Define parameters (ensure max_seq_length and batch_size are defined)\nmax_seq_length = 128  # Example value, adjust as needed\nbatch_size = 64      # Example value, adjust as needed\nnum_epochs = 4\nlearning_rate = 1e-4\nweight_decay = 0.01\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n\n# Load tokenizer\ntokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n\n# Load dataset\nwiki_text_dataset = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\")\n\n# Tokenize dataset\ndef tokenize_function(examples):\n    return tokenizer(examples[\"text\"], truncation=True, padding='max_length', max_length=max_seq_length)\ntokenized_train_dataset = wiki_text_dataset[\"train\"].map(tokenize_function, batched=True)\n\n# Prepare MLM data\ndef prepare_mlm_data(examples):\n    input_ids = torch.tensor(examples[\"input_ids\"])\n    batch_size_local = len(input_ids)\n    sequence_length = len(input_ids[0])\n    mask_prob = 0.15\n    mask_token_id = tokenizer.mask_token_id\n    pad_token_id = tokenizer.pad_token_id\n    cls_token_id = tokenizer.cls_token_id\n    sep_token_id = tokenizer.sep_token_id\n\n    rand = torch.rand(batch_size_local, sequence_length)\n    mask = (rand < mask_prob) & (input_ids != cls_token_id) & (input_ids != sep_token_id) & (input_ids != pad_token_id)\n\n    labels = input_ids.clone()\n    labels[~mask] = -100\n    inputs = input_ids.clone()\n    inputs[mask] = mask_token_id\n\n    return {\"input_ids\": inputs, \"labels\": labels}\nmlm_tokenized_train_dataset = tokenized_train_dataset.map(\n    prepare_mlm_data,\n    batched=True,\n    remove_columns=[\"text\", \"token_type_ids\"]\n)\n\n# Prepare SOP data\nimport re\nfrom datasets import Dataset as HFDataset\n\ndef split_into_sentences_robust(text):\n    sentence_pattern = r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?|!)\\s+'\n    sentences = re.split(sentence_pattern, text)\n    return [s.strip() for s in sentences if s.strip()]\n\nsop_texts_limited = []\nsop_labels_limited = []\ncls_token = tokenizer.cls_token\nsep_token = tokenizer.sep_token\nmax_pairs_per_document = 5\n\nfor example in wiki_text_dataset[\"train\"]:\n    text = example[\"text\"]\n    sentences = split_into_sentences_robust(text)\n    pairs_count = 0\n    for i in range(len(sentences) - 1):\n        if pairs_count >= max_pairs_per_document:\n            break\n        sentence_a = sentences[i]\n        sentence_b = sentences[i + 1]\n        sop_texts_limited.append(cls_token + \" \" + sentence_a + \" \" + sep_token + \" \" + sentence_b)\n        sop_labels_limited.append(0)\n        pairs_count += 1\n        if pairs_count >= max_pairs_per_document:\n            break\n        sop_texts_limited.append(cls_token + \" \" + sentence_b + \" \" + sep_token + \" \" + sentence_a)\n        sop_labels_limited.append(1)\n        pairs_count += 1\n\nsop_dataset = HFDataset.from_dict({\"text\": sop_texts_limited, \"sentence_order_labels\": sop_labels_limited})\n\ndef tokenize_sop_function(examples):\n    return tokenizer(examples[\"text\"], truncation=True, padding='max_length', max_length=max_seq_length)\ntokenized_sop_dataset = sop_dataset.map(tokenize_sop_function, batched=True)\n\n# Set format to PyTorch tensors\nmlm_tokenized_train_dataset.set_format(\"torch\")\ntokenized_sop_dataset.set_format(\"torch\")\n\n# DataLoader for MLM dataset\nmlm_dataloader = DataLoader(\n    mlm_tokenized_train_dataset,\n    batch_size=batch_size,\n    shuffle=True\n)\n# DataLoader for SOP dataset\nsop_dataloader = DataLoader(\n    tokenized_sop_dataset,\n    batch_size=batch_size,\n    shuffle=True\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T11:58:37.384292Z","iopub.execute_input":"2025-03-29T11:58:37.384609Z","iopub.status.idle":"2025-03-29T12:11:43.682890Z","shell.execute_reply.started":"2025-03-29T11:58:37.384586Z","shell.execute_reply":"2025-03-29T12:11:43.681815Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1801350 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"731f32ab851a414b8c23b3d1bca046ff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3189245 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cbd0540941af41e1be1a060ad1844a9c"}},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"modelpretrain.to(device)\nfrom torch.optim import AdamW\noptimizer = AdamW(modelpretrain.parameters(), lr=learning_rate, weight_decay=weight_decay)\nwandb.watch(modelpretrain)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T13:33:13.528585Z","iopub.execute_input":"2025-03-29T13:33:13.528884Z","iopub.status.idle":"2025-03-29T13:33:13.579478Z","shell.execute_reply.started":"2025-03-29T13:33:13.528860Z","shell.execute_reply":"2025-03-29T13:33:13.578770Z"}},"outputs":[],"execution_count":51},{"cell_type":"code","source":" for epoch in range(num_epochs):\n    modelpretrain.train()\n    total_mlm_loss = 0\n    total_sop_loss = 0\n    total_steps = min(len(mlm_dataloader), len(sop_dataloader))\n\n    progress_bar = tqdm(range(total_steps), desc=f\"Epoch {epoch+1}/{num_epochs}\")\n\n    for step in progress_bar:\n        try:\n            batch_mlm = next(iter(mlm_dataloader))\n        except StopIteration:\n            print(\"MLM dataloader finished early.\")\n            break\n        try:\n            batch_sop = next(iter(sop_dataloader))\n        except StopIteration:\n            print(\"SOP dataloader finished early.\")\n            break\n\n        # Move MLM batch to device\n        mlm_inputs = {k: batch_mlm[k].to(device) for k in batch_mlm}\n\n        # Move SOP batch to device\n        sop_inputs = {\n                    k: v.to(device) if isinstance(v, torch.Tensor) else v\n                    for k, v in batch_sop.items()\n                    }\n\n        # Zero gradients\n        optimizer.zero_grad()\n\n        # MLM forward pass\n        returned_loss, prediction_logits_mlm, prediction_logits_sop = modelpretrain(\n            input_ids=mlm_inputs['input_ids'],\n            attention_mask=mlm_inputs['attention_mask'],\n            masked_labels=mlm_inputs['labels']\n        )\n\n        mlm_loss = returned_loss # Now you can use 'returned_loss' which holds the total loss\n\n        # SOP forward pass\n        outputs_sop = modelpretrain(**{\n            'input_ids': sop_inputs['input_ids'],\n            'attention_mask': sop_inputs['attention_mask'],\n            'token_type_ids' : sop_inputs.get('token_type_ids', torch.tensor([])),\n            'sentence_order_labels': sop_inputs['sentence_order_labels']\n        })\n        sop_loss = outputs_sop[0]\n\n        # Combine losses\n        total_loss = mlm_loss + sop_loss\n\n        # Backward pass\n        total_loss.backward()\n\n        # Update parameters\n        optimizer.step()\n\n        total_mlm_loss += mlm_loss.item()\n        total_sop_loss += sop_loss.item()\n        wandb.log({\"mlm_loss\": mlm_loss.item(), \"sop_loss\": sop_loss.item(), \"total_loss\": total_loss.item()})\n        progress_bar.set_postfix(mlm_loss=f\"{mlm_loss.item():.4f}\", sop_loss=f\"{sop_loss.item():.4f}\")\n        \n    avg_mlm_loss = total_mlm_loss / total_steps if total_steps > 0 else 0\n    avg_sop_loss = total_sop_loss / total_steps if total_steps > 0 else 0\n    run.log({\"Total Unsupervised Loss\": total_loss})\n    save_path = '/kaggle/working/pretrained_model.pth'  # Replace with your desired path\n    wandb.log({\"avg_mlm_loss\": avg_mlm_loss, \"avg_sop_loss\": avg_sop_loss, \"epoch\": epoch + 1})\n    # Save the state_dict of the model\n    torch.save(modelpretrain.state_dict(), save_path)\n    wandb.save(save_path)\n    print(f\"Pretrained model weights saved to: {save_path}\")\n    print(f\"Epoch {epoch+1}/{num_epochs} finished, Average MLM Loss: {avg_mlm_loss:.4f}, Average SOP Loss: {avg_sop_loss:.4f}\")\n\nprint(\"Pretraining finished!\")\nrun.finish()","metadata":{"execution":{"iopub.status.busy":"2025-03-29T13:34:07.260865Z","iopub.execute_input":"2025-03-29T13:34:07.261219Z","iopub.status.idle":"2025-03-29T13:36:41.388182Z","shell.execute_reply.started":"2025-03-29T13:34:07.261191Z","shell.execute_reply":"2025-03-29T13:36:41.387028Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"Epoch 1/4:   0%|          | 0/28147 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"37f42b4eff97464b92c101d94b4013c2"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-53-5fa92d2a4b12>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m            \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m        \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m            \u001b[0mbatch_sop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msop_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m        \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m            \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SOP dataloader finished early.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    699\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             if (\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 756\u001b[0;31m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    757\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_index\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 691\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/sampler.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    345\u001b[0m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m             \u001b[0midx_in_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msampler\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    348\u001b[0m                 \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx_in_batch\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m                 \u001b[0midx_in_batch\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/sampler.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandperm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m             yield from torch.randperm(n, generator=generator).tolist()[\n\u001b[1;32m    199\u001b[0m                 \u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":53},{"cell_type":"code","source":"save_path = '/kaggle/working/pretrained_model.pth'  # Replace with your desired path\ntorch.save(modelpretrain.state_dict(), save_path)\nwandb.save(save_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T13:03:20.522347Z","iopub.execute_input":"2025-03-29T13:03:20.522636Z","iopub.status.idle":"2025-03-29T13:03:20.776184Z","shell.execute_reply.started":"2025-03-29T13:03:20.522613Z","shell.execute_reply":"2025-03-29T13:03:20.775526Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n","output_type":"stream"},{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"['/kaggle/working/wandb/run-20250329_113127-z8vylzsa/files/working/pretrained_model.pth']"},"metadata":{}}],"execution_count":28},{"cell_type":"code","source":"class AtomBERTForSequenceClassification(nn.Module):\n    def __init__(self, pretrained_config, num_classes):\n        super().__init__()\n        self.bert = AtomBERT(\n            vocab_size=pretrained_config.vocab_size,\n            embedding_dim=pretrained_config.embedding_dim,\n            hidden_size=pretrained_config.hidden_size,\n            num_layers=pretrained_config.num_layers,\n            num_attention_heads=pretrained_config.num_attention_heads,\n            intermediate_size=pretrained_config.intermediate_size,\n            num_classes=num_classes, # For MNLI this will be 3\n            max_seq_length=pretrained_config.max_seq_length,\n            dropout_rate=pretrained_config.dropout_rate,\n            type_vocab_size=pretrained_config.type_vocab_size if hasattr(pretrained_config, 'type_vocab_size') else 2\n        )\n        self.dropout = nn.Dropout(pretrained_config.dropout_rate)\n        self.classifier = nn.Linear(pretrained_config.hidden_size, num_classes)\n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.bert(input_ids, attention_mask)\n        pooled_output = outputs[1] # We typically use the pooled output for classification\n        pooled_output = self.dropout(pooled_output)\n        logits = self.classifier(pooled_output)\n        return logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T13:03:56.516233Z","iopub.execute_input":"2025-03-29T13:03:56.516532Z","iopub.status.idle":"2025-03-29T13:03:56.522767Z","shell.execute_reply.started":"2025-03-29T13:03:56.516512Z","shell.execute_reply":"2025-03-29T13:03:56.522035Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"\n\n# Define the path to your saved pretrained weights file\npretrained_weights_path = '/kaggle/working/pretrained_model.pth' # Replace with the actual path to your saved file\n\n# Load the configuration of your pretrained model\n# Make sure these values match what you used for pretraining\npretrained_config = AtomBERTConfig(\n    vocab_size=tokenizer.vocab_size,\n    embedding_dim=128,\n    hidden_size=768,\n    num_layers=4,\n    num_attention_heads=4,\n    intermediate_size=1500,\n    max_seq_length=128,\n    dropout_rate=0.1,\n    type_vocab_size=2\n)\n\n# Instantiate the model for sequence classification (for MNLI with 3 classes)\nnum_classes_mnli = 3\nmodel_for_mnli = AtomBERTForSequenceClassification(pretrained_config, num_classes_mnli)\n\n# Load the state dictionary of the pretrained model\npretrained_state_dict = torch.load(pretrained_weights_path, map_location=torch.device(device)) # Ensure device consistency\n\n# Create a new dictionary to store only the 'bert' weights\npretrained_bert_state_dict = {}\nfor name, param in pretrained_state_dict.items():\n    if name.startswith('bert.'):\n        pretrained_bert_state_dict[name[len('bert.'):]] = param\n\n# Load the pretrained weights into the 'bert' part of the sequence classification model\nmodel_for_mnli.bert.load_state_dict(pretrained_bert_state_dict)\n\nprint(\"Pretrained weights loaded successfully into the sequence classification model.\")\n\n# Move the fine-tuning model to the device\nmodel_for_mnli.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T13:04:02.906420Z","iopub.execute_input":"2025-03-29T13:04:02.906709Z","iopub.status.idle":"2025-03-29T13:04:03.123194Z","shell.execute_reply.started":"2025-03-29T13:04:02.906687Z","shell.execute_reply":"2025-03-29T13:04:03.122503Z"}},"outputs":[{"name":"stdout","text":"Pretrained weights loaded successfully into the sequence classification model.\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-30-6f785b1d3dda>:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  pretrained_state_dict = torch.load(pretrained_weights_path, map_location=torch.device(device)) # Ensure device consistency\n","output_type":"stream"},{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"AtomBERTForSequenceClassification(\n  (bert): AtomBERT(\n    (embedding): FactorizedEmbedding(\n      (word_embeddings): Embedding(30522, 128)\n      (token_type_embeddings): Embedding(2, 128)\n      (projection): Linear(in_features=128, out_features=768, bias=True)\n    )\n    (positional_encoding): PositionalEncoding(\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder_layer): TransformerEncoderLayer(\n      (self_attention): MultiHeadSelfAttention(\n        (query): Linear(in_features=768, out_features=768, bias=True)\n        (key): Linear(in_features=768, out_features=768, bias=True)\n        (value): Linear(in_features=768, out_features=768, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (output): Linear(in_features=768, out_features=768, bias=True)\n      )\n      (feed_forward): FeedForwardNetwork(\n        (dense1): Linear(in_features=768, out_features=1500, bias=True)\n        (relu): ReLU()\n        (dense2): Linear(in_features=1500, out_features=768, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=3, bias=True)\n)"},"metadata":{}}],"execution_count":30},{"cell_type":"code","source":"model_finetune = AtomBERTForSequenceClassification(pretrained_config, 3)\nmodel_finetune.to(device)\n# 5. Load the pretrained weights into the new model\n# Load the state dictionary of the pretrained model\npretrained_state_dict = torch.load(pretrained_weights_path, map_location=torch.device(device)) # Ensure device consistency\n\n# Create a new dictionary to store only the 'bert' weights\npretrained_bert_state_dict = {}\nfor name, param in pretrained_state_dict.items():\n    if name.startswith('bert.'):\n        pretrained_bert_state_dict[name[len('bert.'):]] = param\n\n# Load the pretrained weights into the 'bert' part of the sequence classification model\nmodel_finetune.bert.load_state_dict(pretrained_bert_state_dict)\nlearning_rate_finetune = 2e-5\noptimizer = AdamW(model_finetune.parameters(), lr=learning_rate_finetune)\nmodel_finetune.to(device)\n# 9. Define the loss function separately\ncriterion = nn.CrossEntropyLoss()\n# 10. Define your training loop (modified for custom model)\nnum_epochs_finetune = 3\nfor epoch in range(num_epochs_finetune):\n    model_finetune.train()\n    total_loss = 0\n    for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs_finetune}\"):\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n        optimizer.zero_grad()\n        outputs = model_finetune(input_ids, attention_mask=attention_mask) # Forward pass without labels\n        logits = outputs # Assuming your model's forward returns the logits directly\n        loss = criterion(logits, labels) # Calculate the loss\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    print(f\"Epoch {epoch+1} Training Loss: {total_loss/len(train_dataloader)}\")\n\n    # Evaluation loop (modified for custom model)\n    model_finetune.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for batch in tqdm(val_dataloader, desc=\"Evaluating\"):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)\n            outputs = model_finetune(input_ids, attention_mask=attention_mask) # Forward pass without labels\n            logits = outputs\n            _, predicted = torch.max(logits, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    accuracy = 100 * correct / total\n    print(f\"Epoch {epoch+1} Validation Accuracy: {accuracy:.2f}%\")\n\nprint(\"Fine-tuning finished!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T13:37:39.202434Z","iopub.execute_input":"2025-03-29T13:37:39.202820Z","iopub.status.idle":"2025-03-29T14:22:37.862567Z","shell.execute_reply.started":"2025-03-29T13:37:39.202776Z","shell.execute_reply":"2025-03-29T14:22:37.861427Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-55-deff463ce58f>:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  pretrained_state_dict = torch.load(pretrained_weights_path, map_location=torch.device(device)) # Ensure device consistency\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 1/3:   0%|          | 0/12272 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9696256668054354886c5c23aa23546d"}},"metadata":{}},{"name":"stdout","text":"Epoch 1 Training Loss: 1.0137920242818037\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/307 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cddcf4279a134a3da851e0548998b3f0"}},"metadata":{}},{"name":"stdout","text":"Epoch 1 Validation Accuracy: 52.40%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 2/3:   0%|          | 0/12272 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"28b3cd1a4a6141e7bba67a4eda114bf2"}},"metadata":{}},{"name":"stdout","text":"Epoch 2 Training Loss: 0.9483893412071848\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/307 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d09a3a1faeb4401c934082f061b4224c"}},"metadata":{}},{"name":"stdout","text":"Epoch 2 Validation Accuracy: 54.66%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 3/3:   0%|          | 0/12272 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d97af60164084a2cb58f9470bc39953c"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-55-deff463ce58f>\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;31m# Assuming your model's forward returns the logits directly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Calculate the loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             )\n\u001b[0;32m--> 581\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    826\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":55}]}